{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae9420de-ccb7-4acf-b4e4-39ccd03196e2",
   "metadata": {},
   "source": [
    "# Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4636bad1-43c7-4186-8e9a-9c8aff9ee470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "\n",
    "class PreTrainedEmbeddings(object):\n",
    "\n",
    "    def __init__(self, word_to_index, word_vectors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word_to_index (dict): mapping from word to integers\n",
    "            word_vectors (list of numpy arrays)\n",
    "        \"\"\"\n",
    "        self.word_to_index = word_to_index\n",
    "        self.word_vectors = word_vectors\n",
    "        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
    "        embedding_dim = len(word_vectors[0])\n",
    "        self.index = AnnoyIndex(embedding_dim, metric=\"euclidean\")\n",
    "        for word, i in self.word_to_index.items():\n",
    "            self.index.add_item(i, self.word_vectors[i])\n",
    "        self.index.build(50)\n",
    "    @classmethod\n",
    "    def from_embedding_file(cls, embedding_file):\n",
    "        \"\"\"\n",
    "        Instantiate from pretrained vector file.\n",
    "\n",
    "        VECTOR FILE SHOULD BE OF THE FORM:\n",
    "        word0 x0_0 x0_1 x0_2 ... x0_N\n",
    "        word1 x1_0 x1_1 x1_2 ... x1_N\n",
    "\n",
    "        Args:\n",
    "            embedding_file (str): location of the file\n",
    "\n",
    "        Returns:\n",
    "            PreTrainedEmbeddings instance\n",
    "        \"\"\"\n",
    "        word_to_index = {}\n",
    "        word_vectors = []\n",
    "        with open(embedding_file, encoding=\"utf-8\") as fp:\n",
    "            for line in fp.readlines():\n",
    "                line = line.split(\" \")\n",
    "                word = line[0]\n",
    "                vec = np.array([float(x) for x in line[1:]])\n",
    "\n",
    "                word_to_index[word] = len(word_to_index)\n",
    "                word_vectors.append(vec)\n",
    "\n",
    "        return cls(word_to_index, word_vectors)\n",
    "    def get_embedding(self,word):\n",
    "        \"\"\"args:\n",
    "        word(str)\n",
    "            Returns an embeding(numyp):()nd array)\n",
    "        \"\"\"\n",
    "        return self.word_vectors[self.word_to_index[word]]\n",
    "    def get_closest_to_vector(self,vector,n=1):\n",
    "        \"\"\"Given a vector, return its n nearest neighbors\n",
    "        args: Vector (np.ndarray): shouldmatch the size of the vectores in the Annoy index\n",
    "        n (int): the number of neighbors to return\n",
    "        Returns:\n",
    "            [str, str , ..]: words nearest to the given vector the words are not ordere by distacne\n",
    "        \"\"\"\n",
    "        nn_indices=self.index.get_nns_by_vector(vector,n)\n",
    "        return [self.index_to_word[neighbor] for neighbor in nn_indices]\n",
    "    def compute_and_print_analogy(self,word1,word2, word3):\n",
    "        \"\"\"\n",
    "            prints the solutions to analogies using word embeddings \n",
    "            analogies are word1 is to word2 as word3 is to _\n",
    "            this method will print : word1 word2 word3 : word4 \n",
    "        Args:\n",
    "            word1 (str)\n",
    "            word2 (str)\n",
    "            word3 (str)\n",
    "        \"\"\"\n",
    "        vec1= self.get_embedding(word1)\n",
    "        vec2 = self.get_embedding(word2)\n",
    "        vec3 = self.get_embedding(word3)\n",
    "        #Simple hypotesis: Analogy is a spatial relationship\n",
    "        spatial_relationship = vec2- vec1\n",
    "        vec4=vec3+ spatial_relationship\n",
    "\n",
    "        closest_words=self.get_closest_to_vector(vec4,n=4)\n",
    "        existing_words = set([word1,word2,word3])\n",
    "        closest_words= [word for word in closest_words if word not in existing_words]\n",
    "\n",
    "        if len(closest_words)==0:\n",
    "            print(\"No se pudo encontrar palabras vecinas para el vector\")\n",
    "            return\n",
    "        for word4 in closest_words:\n",
    "            print(f\"{word1}:{word2}:{word3}:{word4}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c2f95ab-d396-424d-90db-82890f5f466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:he:woman:she:\n",
      "man:he:woman:her:\n"
     ]
    }
   ],
   "source": [
    "embeddings=PreTrainedEmbeddings.from_embedding_file(\"data/wiki_giga_2024_300_MFT20_vectors_seed_2024_alpha_0.75_eta_0.05_combined.txt\")\n",
    "embeddings.compute_and_print_analogy(\"man\",\"he\",\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc1e0bd7-68ff-4d00-88ed-bede1342349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly:plane:sail:ship:\n",
      "fly:plane:sail:vessel:\n",
      "fly:plane:sail:boat:\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"fly\",\"plane\",\"sail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56356bce-fd12-4d34-b776-5ae428443fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat:kitten:dog:puppy:\n",
      "cat:kitten:dog:rottweiler:\n",
      "cat:kitten:dog:hound:\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"cat\",\"kitten\",\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fced83c-ee42-4181-be33-b32c9d4044fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue:color:dog:pet:\n",
      "blue:color:dog:taste:\n",
      "blue:color:dog:cartoon:\n",
      "blue:color:dog:introduces:\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"blue\",\"color\",\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74c90949-7fe7-4140-b0bd-a1d82021ff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toe:foot:finger:inside:\n",
      "toe:foot:finger:turned:\n",
      "toe:foot:finger:apart:\n",
      "toe:foot:finger:moving:\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"toe\",\"foot\",\"finger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63074d57-3a98-4b8c-af68-cc5b5d2e6f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk:communicate:read:instructions:\n",
      "talk:communicate:read:translated:\n",
      "talk:communicate:read:accurately:\n",
      "talk:communicate:read:identify:\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"talk\",\"communicate\",\"read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0faaea35-b13a-4cbf-ad97-a21439ebf513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue:democrat:red:democratic:\n",
      "blue:democrat:red:senator:\n",
      "blue:democrat:red:republican:\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"blue\",\"democrat\",\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3177a4bc-4ed0-4bbb-9cee-d14dabfbe108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast:fastest:young:youngest:\n",
      "fast:fastest:young:sixth:\n",
      "fast:fastest:young:fourth:\n",
      "fast:fastest:young:fifth:\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"fast\",\"fastest\",\"young\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a010a992-837f-4783-adf1-b6f42b3bfa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans:lesbian:gay:lgbt:\n",
      "trans:lesbian:gay:transgender:\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"trans\",\"lesbian\",\"gay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "326452c0-4a75-4ed3-a08f-bf3d6cf3b661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science:computer:ai:user:\n",
      "science:computer:ai:lets:\n",
      "science:computer:ai:programmed:\n",
      "science:computer:ai:iphone:\n"
     ]
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"science\",\"computer\",\"ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451d6d5-97b6-4df5-8056-dcf4a94e039f",
   "metadata": {},
   "source": [
    "## Continous Bag Of Words Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f0de36e5-1923-4fe9-87db-fc7b943dc3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "class CBOWDataset(Dataset):\n",
    "    # Se hereda la clase DataseT\n",
    "    def __init__(self, cbow_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (CBOWVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.cbow_df = cbow_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "        \n",
    "        self.train_df = self.cbow_df[self.cbow_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.cbow_df[self.cbow_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.cbow_df[self.cbow_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls,cbow_cls):\n",
    "        \"\"\"\n",
    "        carga el dataset y hacer un nuevo vectorizador desde cero\n",
    "        args:\n",
    "                cbow_cvs(str): ubicacion del dataset\n",
    "        Returns:\n",
    "            una instancia de ReviewDataset\n",
    "        \"\"\"\n",
    "        cbow_df= pd.read_csv(cbow_cls)\n",
    "        train_cbow_df=cbow_df[cbow_df.split==\"train\"]\n",
    "        return cls(cbow_df,CBOWVectorizer.from_dataframe(train_cbow_df))\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"retorna el vectorizador\"\"\"\n",
    "        return self._vectorizer\n",
    "    def set_split(self,split=\"train\"):\n",
    "        \"\"\"\n",
    "        Selecciona la division en el conjunto de datos usando una columna en el dataframe\n",
    "        args:\n",
    "        split(str):  uno de \"train\",\"val\",\"test\"\n",
    "        \"\"\"\n",
    "        self._target_split=split\n",
    "        self._target_df,self._target_size=self._lookup_dict[split]\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"\n",
    "        El punto primario de entrada como metodo para PyTorch en lso conjuntos de datos\n",
    "        Args:\n",
    "        index(int): Es el inice del dato\n",
    "        Returns:\n",
    "            Diccionario de las caracterizitcads de los puntos de datos y labels\n",
    "        \"\"\"\n",
    "        row=self._target_df.iloc[index]\n",
    "        context_vector= self._vectorizer.vectorize(row.context,self._max_seq_length)\n",
    "        target_index = self._vectorizer.cbow_vocab.lookup_token(row.target)\n",
    "        return {'x_data':context_vector,\n",
    "               'y_data':target_index}\n",
    "    def get_num_batches(self,batch_size):\n",
    "        \"\"\"\n",
    "        Dado un tamaño de batch , retorna el numero de batches del conjunto de datos\n",
    "        Args:\n",
    "            batch_size(int)\n",
    "        Returns:\n",
    "            Numero de batches en el conjuntos\n",
    "        \"\"\"\n",
    "        return len(self)//batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880da8a8-2684-4f90-986a-9bfb92bb0c43",
   "metadata": {},
   "source": [
    "## Vocabulario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e41723b-4b08-4acd-8127-53e754c0c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" Clase a procesar texto y extrar el vocabulario para mappear\"\"\"\n",
    "    def __init__(self,token_to_idx=None,mask_token=\"<MASK>\",add_unk=True,unk_token=\"<UNK>\"):\n",
    "        \"\"\"Args:\n",
    "            token_to_idx (dict): un mapa pre existen de toknes a indices\n",
    "            add_unk(bool): un bandera que indica si se añade el token UNK de desconocido\n",
    "            unk_token(str): el token UNK se añade a el vocabulario\n",
    "            \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx={}\n",
    "        self._token_to_idx=token_to_idx\n",
    "        self._idx_to_token={idx: token\n",
    "                           for token,idx in self._token_to_idx.items()}\n",
    "        self._add_unk=add_unk\n",
    "        self._unk_token=unk_token\n",
    "        self._mask_token=mask_token\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index=-1\n",
    "        if add_unk:\n",
    "            self.unk_index=self.add_token(unk_token)\n",
    "    def to_serializable(self):\n",
    "        \"\"\" Retorna un diccionario que puede ser serializado \"\"\"\n",
    "        return {\"token_to_idx\":self._token_to_idx,\n",
    "               \"add_unk\":self._add_unk,\n",
    "               \"unk_token\":self._unk_token,\n",
    "               \"mask_token\":self._mask_token}\n",
    "    @classmethod\n",
    "    def from_serializable(cls,contents):\n",
    "        \"\"\"instancia el bocabulario desde un diccionario serializado\"\"\"\n",
    "        return cls(**contents)\n",
    "    def add_token(self,token):\n",
    "        \"\"\"\n",
    "        Actualiza los mapeos de diccionarrios basados en los tokens\n",
    "        Args:\n",
    "            token (str): el item a añadir en el vocabulario\n",
    "        Returns:\n",
    "            index(int) : el entero correspondiente al token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index=self._token_to_idx[token]\n",
    "        else:\n",
    "            index=len(self._token_to_idx)\n",
    "            self._token_to_idx[token]=index\n",
    "            self._idx_to_token[index]=token\n",
    "        return index\n",
    "    def lookup_token(self,token):\n",
    "        \"\"\" Obtiene el indice asociado con el tokenn de UNK token si el token no está presente,\n",
    "        Args:\n",
    "            token (str): el token a observar\n",
    "        returns :\n",
    "            index (int) el indices correspondiente al token\n",
    "        Notes:\n",
    "               `unk_index` necesita ser entero positivo (habiendo sido añadido al vocabulario) para la funcionalidad desconocidad\n",
    "               \"\"\"\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token,self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    def lookup_index(self,index):\n",
    "        \"\"\" Obtiene el token asociado al indice\n",
    "        Args:\n",
    "            index (int): el indice a observar\n",
    "        returns :\n",
    "            token (str) el token correspondiendte al indices\n",
    "        Raises:\n",
    "            KeyError: si el indice no está en el vocabulario\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"El indice (%d)no está en el vocabulario\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    def __len__(self):#Refleja la longitudad del vocabulario\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36a52a-cad9-4ee9-bbf8-fe84b1c6275c",
   "metadata": {},
   "source": [
    "## El vectorizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e6a5a08c-f453-42ac-ac64-3174facc635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "class CBOWVectorizer(object):\n",
    "    \"\"\" La clase vectorizer de las cuales, suss coordenadas son las del vocabulario\"\"\"\n",
    "    def __init__(self,cbow_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_vocab(Vocabulary): mapea las palabras a los enteros\n",
    "            rating_vocab (Vocabulary); Mapea las etiquetas de las clases a enteros\n",
    "            \"\"\"\n",
    "        self.cbow_vocab= cbow_vocab\n",
    "    def vectorize(self,context,vector_length=-1):\n",
    "        \"\"\" \n",
    "        Crea un vecctor colapsado para la reseña\n",
    "        Args:\n",
    "            review(str) : una reseña\n",
    "        Returns:\n",
    "            one_hot (np.ndarray): la codificacion colapsadad \n",
    "        \"\"\"\n",
    "        indices= [self.cbow_vocab.lookup_token(token) for token in context.split(\" \")]\n",
    "        if vector_length<0:\n",
    "            vector_length=len(indices)\n",
    "        out_vector = np.zeros(vector_length,dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.cbow_vocab.mask_index\n",
    "        return out_vector\n",
    "    @classmethod\n",
    "    def from_dataframe(cls,cbow_df):\n",
    "        \"\"\"\n",
    "        Instancia el vectorizer desde el conjunto de datos del dataframe\n",
    "\n",
    "        Args:\n",
    "            review_df (pandas.Dataframe): el conjunto de datos de reseñas\n",
    "            cuttof(int): el parametro para el filtro de basado en frecuencia\n",
    "        Returns\n",
    "        una instacia del ReviewVectorizer\n",
    "        \"\"\"\n",
    "        cbow_vocab=Vocabulary()\n",
    "        #Añada la palabras top if count>providad count\n",
    "        words_counts=Counter()\n",
    "        for index, row in cbow_df.iterrows():\n",
    "            for token in row.context.split(\" \"):\n",
    "                cbow_vocab.add_token(token)\n",
    "            cbow_vocab.add_token(row.target)\n",
    "        return cls(cbow_vocab)\n",
    "    @classmethod \n",
    "    def from_serializable(cls,contents):\n",
    "        \"\"\"\n",
    "        Instancia una ReviewCectorizer desde un diccionario serializavle\n",
    "        Args:\n",
    "                contents(dicT): el diccionario serializable\n",
    "        Returns:\n",
    "            Una instancia de ReviewVectorizer class\n",
    "        \"\"\"\n",
    "        cbow_vocab=Vocabulary.from_serializable(contents[\"cbow_vocab\"])\n",
    "        return cls(cbow_vocab)\n",
    "    def to_serializable(self):\n",
    "        \"\"\"Crea un diccionario seralizable para el chace\n",
    "        Returns:\n",
    "            contents(ditc): el diccionario serializable\n",
    "        \"\"\"\n",
    "        return {\"review_vocab\": self.review_vocab.to_serializable(),\n",
    "               \"rating_vocab\": self.rating_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "360a9049-ce56-4dd0-9abc-9ccbbc3d15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "class CBOWClassifier(nn.Module):\n",
    "    def __init__(self,vocabulary_size,embedding_size, padding_idx=0):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "            vocabulary_size (int): number of vocabulary items, controls the number of embeddings and prediction vector size\n",
    "            embedding_size (int). size of the embeddings\n",
    "            padding_idx (int: default 0; Embedg¿ding will not use this index\n",
    "            \"\"\"\n",
    "        super(CBOWClassifier,self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
    "                                        embedding_dim=embedding_size,\n",
    "                                        padding_idx=padding_idx)\n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                                out_features=vocabulary_size)\n",
    "    def forward(self,x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        Args: \n",
    "            x_in (torch.Tensor):  an input data tensor\n",
    "        x_in.sape should be (batch,input_dim)\n",
    "        apply_softmax (bool): a flag for the softmax activation should be false if used with the cross entropy lossess\n",
    "        Returns:\n",
    "        The resulting tensor, tensor.shape should be (batch,output_dim)\n",
    "        \"\"\"\n",
    "        x_embedded_sum = self.embedding(x_in).sum(dim=1)\n",
    "        y_out= self.fc1(x_embedded_sum)\n",
    "        if apply_softmax:\n",
    "            y_out= F.softmax(y_out,dim=1)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec9764b6-a8c0-43bd-8cce-d64587c98339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            }\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "def generate_batches(dataset,batch_size,shuffle=True,\n",
    "                     drop_last=True,device=\"cpu\"):\n",
    "    \"\"\"Una funcion generation la que wrapea ek dataloader de pytorxh. SE asegurarar de que cada tesnor este en un unico dispositivo\n",
    "    \"\"\"\n",
    "    dataloader=DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e29cc1ae-bbf4-46e9-8b61-e52df7df5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    cbow_csv=\"data/books/frankenstein_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/ch5/cbow\",\n",
    "    # Model hyper parameters\n",
    "    embedding_size=300,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.0001,\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime options\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9549ed0-82e8-4419-b435-2f31c09f3e80",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aa92e3f2-524e-4430-a4f8-6a32e0ca0d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state=make_train_state(args)\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda=False\n",
    "args.device= torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "#Dataset and vectorizer\n",
    "dataset=CBOWDataset.load_dataset_and_make_vectorizer(args.cbow_csv)\n",
    "vectorizer=dataset.get_vectorizer()\n",
    "#Model\n",
    "classifier = CBOWClassifier(vocabulary_size=len(vectorizer.cbow_vocab),embedding_size=args.embedding_size)\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "#Loss and optimizer\n",
    "loss_func= nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(classifier.parameters(),lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1e6de915-010c-4430-a5e1-d72deb59a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index']=epoch_index\n",
    "    # Itera sobre nuestro dataset\n",
    "    #Configura: el generador de batch, la perdidad, el accuracy a 0 y el conjunto de entrenamiento a activo\n",
    "    dataset.set_split(\"train\")\n",
    "    batch_generator=generate_batches(dataset,batch_size=args.batch_size,device=args.device)\n",
    "    running_loss=0.0\n",
    "    running_acc=0.0\n",
    "    classifier.train()\n",
    "    for batch_index,batch_dict in enumerate(batch_generator):\n",
    "        \n",
    "        # La rutina de entrenamiento se conforma de 5 pasos\n",
    "\n",
    "        #paso 1, los gradientes en cero\n",
    "        optimizer.zero_grad()\n",
    "        #Paso . calcula la salida\n",
    "        y_pred=classifier(x_in=batch_dict[\"x_data\"])\n",
    "        #paso 3, calcula la perdidad\n",
    "        loss=loss_func(y_pred,batch_dict[\"y_data\"])\n",
    "        loss_batch=loss.item()\n",
    "        running_loss+=(loss_batch-running_loss)/(batch_index+1)\n",
    "        #paso 4, usa la perdidad para producir el gradiente\n",
    "        loss.backward()\n",
    "        #Paso 5, usa el otpimizar para que tomo el paso gradiente\n",
    "        optimizer.step()\n",
    "\n",
    "        ##Calcula el accuracy\n",
    "        acc_batch=compute_accuracy(y_pred,batch_dict[\"y_data\"])\n",
    "        running_acc+=(acc_batch-running_acc)/(batch_index+1)\n",
    "    train_state[\"train_loss\"].append(running_loss)\n",
    "    train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "    #Iterar sobre el dataset de validacion\n",
    "    #Configura, el generator de batch, la perdiddad, el accuracy a 0 y modo eval\n",
    "    dataset.set_split(\"val\")\n",
    "    batch_generator=generate_batches(dataset,batch_size=args.batch_size,device=args.device)\n",
    "    running_loss=0.0\n",
    "    running_acc=0.0\n",
    "    classifier.eval()\n",
    "    #Empieza la evaluacion\n",
    "    for batch_index,batch_dict in enumerate(batch_generator):\n",
    "        # Paso 1: calcula la salida\n",
    "        y_pred=classifier(x_in=batch_dict[\"x_data\"])\n",
    "        # paso 2, calcula la salida\n",
    "        loss = loss_func(y_pred,batch_dict['y_data'])\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch-running_loss)/(batch_index+1)\n",
    "        #paso 3, calcula el accuracy\n",
    "        acc_batch= compute_accuracy(y_pred,batch_dict['y_data'])\n",
    "        running_acc+=(acc_batch-running_acc)/(batch_index+1)\n",
    "    train_state[\"val_loss\"].append(running_loss)\n",
    "    train_state[\"val_acc\"].append(running_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "44e33d78-377c-4755-8fce-76efd2e6aa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comienza la evaluacion\n",
      "batch:  0\n",
      "batch:  1\n",
      "batch:  2\n",
      "batch:  3\n",
      "batch:  4\n",
      "batch:  5\n",
      "batch:  6\n",
      "batch:  7\n",
      "batch:  8\n",
      "batch:  9\n",
      "batch:  10\n",
      "batch:  11\n",
      "batch:  12\n",
      "batch:  13\n",
      "batch:  14\n",
      "batch:  15\n",
      "batch:  16\n",
      "batch:  17\n",
      "batch:  18\n",
      "batch:  19\n",
      "batch:  20\n",
      "batch:  21\n",
      "batch:  22\n",
      "batch:  23\n",
      "batch:  24\n",
      "batch:  25\n",
      "batch:  26\n",
      "batch:  27\n",
      "batch:  28\n",
      "batch:  29\n",
      "batch:  30\n",
      "batch:  31\n",
      "batch:  32\n",
      "batch:  33\n",
      "batch:  34\n",
      "batch:  35\n",
      "batch:  36\n",
      "batch:  37\n",
      "batch:  38\n",
      "batch:  39\n",
      "batch:  40\n",
      "batch:  41\n",
      "batch:  42\n",
      "batch:  43\n",
      "batch:  44\n",
      "batch:  45\n",
      "batch:  46\n",
      "batch:  47\n",
      "batch:  48\n",
      "batch:  49\n",
      "batch:  50\n",
      "batch:  51\n",
      "batch:  52\n",
      "batch:  53\n",
      "batch:  54\n",
      "batch:  55\n",
      "batch:  56\n",
      "batch:  57\n",
      "batch:  58\n",
      "batch:  59\n",
      "batch:  60\n",
      "batch:  61\n",
      "batch:  62\n",
      "batch:  63\n",
      "batch:  64\n",
      "batch:  65\n",
      "batch:  66\n",
      "batch:  67\n",
      "batch:  68\n",
      "batch:  69\n",
      "batch:  70\n",
      "batch:  71\n",
      "batch:  72\n",
      "batch:  73\n",
      "batch:  74\n",
      "batch:  75\n",
      "batch:  76\n",
      "batch:  77\n",
      "batch:  78\n",
      "batch:  79\n",
      "batch:  80\n",
      "batch:  81\n",
      "batch:  82\n",
      "batch:  83\n",
      "batch:  84\n",
      "batch:  85\n",
      "batch:  86\n",
      "batch:  87\n",
      "batch:  88\n",
      "batch:  89\n",
      "batch:  90\n",
      "batch:  91\n",
      "batch:  92\n",
      "batch:  93\n",
      "batch:  94\n",
      "batch:  95\n",
      "batch:  96\n",
      "batch:  97\n",
      "batch:  98\n",
      "batch:  99\n",
      "batch:  100\n",
      "batch:  101\n",
      "batch:  102\n",
      "batch:  103\n",
      "batch:  104\n",
      "batch:  105\n"
     ]
    }
   ],
   "source": [
    "dataset.set_split(\"test\")\n",
    "batch_generator=generate_batches(dataset,batch_size=args.batch_size,device=args.device)\n",
    "running_loss=0.\n",
    "running_acc=0.\n",
    "classifier.eval()\n",
    "print(\"comienza la evaluacion\")\n",
    "for batch_index,batch_dict in enumerate(batch_generator):\n",
    "    print(\"batch: \",batch_index)\n",
    "    #Compute the output\n",
    "    y_pred=classifier(x_in=batch_dict[\"x_data\"])\n",
    "    # calcula la perdidad\n",
    "    loss = loss_func(y_pred,batch_dict[\"y_data\"])\n",
    "    loss_batch= loss.item()\n",
    "    running_loss += (loss_batch-running_loss)/(batch_index+1)\n",
    "    #Calcula el accuracy \n",
    "    acc_batch= compute_accuracy(y_pred,batch_dict[\"y_data\"])\n",
    "    running_acc+=(acc_batch-running_acc)/(batch_index+1)\n",
    "train_state[\"test_loss\"]= running_loss\n",
    "train_state[\"test_acc\"] = running_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f26e0-e1da-4a6c-9408-c307b2d015a9",
   "metadata": {},
   "source": [
    "## Este modelo, solo tiene el detalle de infereir la siguiente palabra\n",
    "> No tiene buen accuracy, debido a que e sun libro con solo 70000 palabras, y los embeddings desde 0 se hacen con cientos de teras, y es necesario, para que un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f79ab-930f-49ce-8b06-4341ee2630b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
