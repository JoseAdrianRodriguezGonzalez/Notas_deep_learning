{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c89534-7bed-4761-ace4-5d4c934838d8",
   "metadata": {},
   "source": [
    "# Como tokenizar texto\n",
    "Un antes usaba expresiones regulares para separar, sin embargo, puedes usar spacy.blank para tokenizar con los espacios blancos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de6366c0-73df-48ec-b955-433d8849ada5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch']\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "nlp=spacy.blank(\"en\")\n",
    "text=\"Mary, don't slap the green witch\"\n",
    "print([str(token) for token in nlp(text.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3cdf2a7-371f-416d-b4a9-1592aa7eb3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':-)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet=\"Snow White and the Seven Degrees\\n#MakeAMovieCold@midnight:-)\" \n",
    "tokenizer =TweetTokenizer()\n",
    "print(tokenizer.tokenize(tweet.lower()))\n",
    "#Utiliza un modelo de NLP que tokeniza cosas comunes un twitter, como hastags, etiquetados y emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac979b-965e-45e0-8cac-5a2f055c2653",
   "metadata": {},
   "source": [
    "## Los tipos\n",
    "> Los tipos son tokens uniocs en un corpus, el conjunto de todos estos tipos en un corpus, es el denominado vocabulario o lexicon \n",
    "Las palabas pueden ser separadas como stopwords y como content words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a7b6d-48a3-4e90-91a5-8e49b00db202",
   "metadata": {},
   "source": [
    "# N grams\n",
    "Son la longitud de tokens consecutivos en una secuencia de un texto. Un bigram contiene  2 tokens, unigram solo 1, y asi sucesivamente \n",
    "Se pueden generar n-grams con NLTK y spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98a4703a-4656-404c-b6cb-ee6dd05d752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['mary', ',', \"n't\", 'slap', 'green', 'witch', '.']\n",
      "por token [['mary', ',', \"n't\"], [',', \"n't\", 'slap'], [\"n't\", 'slap', 'green'], ['slap', 'green', 'witch'], ['green', 'witch', '.']]\n"
     ]
    }
   ],
   "source": [
    "def n_grams(text,n):\n",
    "    \"\"\"\n",
    "    Toma toknes de texto y retorma una lista de n-grams\n",
    "    \"\"\"\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "original=[\"mary\",\",\",\"n't\",\"slap\",\"green\",\"witch\",\".\"]\n",
    "print(f\"Original: {original}\")\n",
    "ngr=n_grams(original,3)\n",
    "print(f\"por token {ngr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa999d9-ba84-43ad-8f25-7a5a0003dcd1",
   "metadata": {},
   "source": [
    "> Esto puede ser particularmente util en casos donde la subpalabra de información, tal es el caso de nomenclatura quimica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b3886-d059-4296-9da7-ae4d2516c7b5",
   "metadata": {},
   "source": [
    "## Lemmas y Stems\n",
    "Los lemmas son la raíz de la palabra, en inglés, un ejemplo es flow, flew, flies,flown flowing, pero todos derivan de fly. Puede ser muy util para reducir la cantidad de tokens que se usa, para esta tarea usaremos el modelo completo, pero pequeño de spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81a6bcba-fee0-4087-a87c-fc1940d1d096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he-----> he\n",
      "was-----> be\n",
      "running-----> run\n",
      "late-----> late\n"
     ]
    }
   ],
   "source": [
    "nlp_lemma=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp_lemma(u\"he was running late\")\n",
    "for token in doc:\n",
    "    print(f\"{token}-----> {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c88461-7f3a-4dfd-ad86-974917f469f4",
   "metadata": {},
   "source": [
    "# Stems\n",
    "> La diferencia es que sua reglas predefinidad para acortar la palabra. Mientras que la lemmatización sabe la raíz de la palabra como por ejemplo, se puede ver el caso de la palabra amar. Amar,amo, amada,amado, dira que es de amar. La stemización solo dira que es am"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1742a-e536-4f04-9d10-d2e349cdb85c",
   "metadata": {},
   "source": [
    "# Categorizando palabras\n",
    "Esta ha sido una de las tareas más primitas del NLP y comunees, consiste en etiquetar cierta cantidad de texto , como por ejemplo, topicos, identificacion del lenguaje y email pueden determinarse como problemas de este nicho "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dad997d0-c04e-4ae6-b246-9c745a5fcb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary- PROPN\n",
      "slapped- VERB\n",
      "the- DET\n",
      "green- PROPN\n",
      "witch- PROPN\n",
      ".- PUNCT\n"
     ]
    }
   ],
   "source": [
    "nlp_tagger=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp_tagger(u\"Mary slapped the green witch.\")\n",
    "for token in doc:\n",
    "    print(f\"{token}- {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c1ab7-6f28-4a91-9193-b34b41af1b6f",
   "metadata": {},
   "source": [
    "> Esta forma de etiquetar, permite identificar cada una de los componentes de una oración, como por pronombres o personas, verbos, el determinante, sustantivos y signos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea730028-4fb9-4a5c-b73b-c17c635cd549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary - NP\n",
      "the green witch - NP\n"
     ]
    }
   ],
   "source": [
    "nlp_categories=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp_categories(u\"Mary slapped the green witch.\")\n",
    "for token in doc.noun_chunks:\n",
    "    print(f\"{token} - {token.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb8852f-e40c-4f78-803d-9a3701d2ef00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
