{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c742891-9971-4999-9aef-1c569fb7587a",
   "metadata": {},
   "source": [
    "# Advanced sequence modeling for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30068d61-323f-4891-97df-4ef11f81f333",
   "metadata": {},
   "source": [
    "\n",
    "En esta sección, se aboradn diversos, tópcios, com es desde los modelos seq2seq que surgen la metodolgoía encoder-decorder, y por consiguiente la alineación de tokebns. Finalmente,técncias de como enteder polisemas.\n",
    "## Encoder-decoder\n",
    "    El primer caso, la metodología de encoder-decoder, permite tener un texto crudo, el modelo codifica este teto para saca alguna caracteristica, y finalmente lo decodicficada a un lenguaje humano interpretable. Un ejemplo sería como el anterior generador de texto, ya que tnemos el modelo RNN-GRU que tienela parte oculta en el tiempo, que es el encoder, y la parte del decorder, estener toda la lista de ancionaldiades para definir que apellidos son de ciertas nacionalidades.\n",
    "## Alineación\n",
    "    Con esta metodolgoía se intena alinear primerarnete los tokebne,s por ejem´plo, cundo un o escribe en un teclado de cleular, podemos tener mutliples repsuestas de cuandoe scribimos casa para escribir en al siguiente frase. Un ejemplo de esto sería, que si escribe la casa, antes ya me habrá escrito una casa.\n",
    "Entonces, esto implica que un texto pude tener mutliples posibles respeustas, y es lo qyue necesita atacr, ¿Como puedo encontrar mutliples respuestas para una sola entrada?, y entramos con analisi bidreccional, porque por ejemplo, para comprender la semántica de  una palabra, podemos emepzar con la primera aprocximación de utilizar la parte anterior a la palabra, ya la sigueinte de la palabra la analizamos tabien,a mbos con modelos de redes recurrentes, entonces, se conatenan los resultados, y decoodificamos el resultado. El rpoblema de esto e s que los modelos recurrentes son poco paralelizables, si bien, es un divide y venceras de priemras, pero no puede ser paralelizado por palabra y necesita comprender el cotnexto completo para dar una respeusta completa sobre una mima palabra.\n",
    "\n",
    "#Atención\n",
    "\n",
    "El termino de atención surge del neuroanálisis, en el que como los humanos podemos comprender las cosas. Por lo general, al leer, no entedemos todos como un string  llano, suino que hay palabras que ñlas marcamos como ideas calves y analizanmos el contexto d e estas palabras para poder comprender us signficado. En traducción, por ejemplo, intetnamos comprender la palabra nnueva o ddesconocida, en base a palabras cirucndantes o que podemos asiganr con matyor peso parapodre comrpedenr el significado o traducciónd e esta palabra.\n",
    "\n",
    "La atención puede urgir de diferents formas, como por ejemplo peude ser que el análisis sea local, o que sea global, pero esto nos permite hac el proceswo paralelzibale a diferencia de los modelos recurrentes.\n",
    "\n",
    "En este colab contendrá una peuqeña idea de la atención y de como itnroducir conceptos clae como query,clave y valor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180c0aa-2753-4aa2-adec-ae29eb94dd31",
   "metadata": {},
   "source": [
    "# maquina de traudcción neuroanl\n",
    "Es una técnica en al que se busca cque comptuacionalemnte puieda traducir un texto de un idioma a otr o idioma,. Esta tarea ha liderado la investigación fuerte con repsecto a la atencióbn, ya que tener un gran compendio de las palabras para pdoer traducir unaa plaabra resulta complicado, ya que muchas traducciones pueden ser más interpretativas que directs ddeundiccionario. \n",
    "\n",
    "Aquí srugen diversas, metricas como BLEU, ROUGE, METEOR.Que miden que tan bien se traduce de un idioma a otro.\n",
    "El perplexity por su parte m ide la probabildiad en que una palabra ´pueda ocurrir dado un evento, esto surge de la teoria de la información y por lo tanto, tiene intrinsecamente la entropía cruzada, como se puede ver en la formula.\n",
    "\n",
    "$$Perplexity(x)=2^{-P(x)\\log P(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c289b-d96d-4793-b31f-3adec492047c",
   "metadata": {},
   "source": [
    "# Preprocessamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e0ba35-92a4-42cf-adf8-20e8851b2153",
   "metadata": {},
   "source": [
    "> Para esta sección, se debe de hacer un preprocesameinto de los datos en crudo\n",
    ">\n",
    "> La primera etapa es definir nuestros párametros con argprase, sucesivamente, leemos ela rchivo de texto y separamos,reemplazamos los saltos de linea, lo hacemos minuscuals y dividmos el onjunto en dos tuplas a traves de \\t, esto porque con \\t esta la separacion entre ingles y frances para cada linea de nuestro arhcivo de texto.\n",
    "> Sucesivamente, filtramos nuestro conjunto con reglas establecidas de palabras que empiezan con pronombres en presente, esto para reducir y convergerel tiempo de entrenamiento den nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f126d4c-432f-4dd6-bcb1-ac8fd9fa3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5768ae6c-14c1-40e3-bd00-aeaaf00eff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    source_data_path=\"data/nmt/eng-fra.txt\",\n",
    "    output_data_path=\"data/nmt/simplest_eng_fra.csv\",\n",
    "    perc_train=0.7,\n",
    "    perc_val=0.15,\n",
    "    perc_test=0.15,\n",
    "    seed=1337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970e5109-c77e-429f-86c0-599cfc9e5816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go.', 'va !'], ['run!', 'cours\\u202f!'], ['run!', 'courez\\u202f!'], ['wow!', 'ça alors\\u202f!'], ['fire!', 'au feu !'], ['help!', \"à l'aide\\u202f!\"], ['jump.', 'saute.'], ['stop!', 'ça suffit\\u202f!'], ['stop!', 'stop\\u202f!'], ['stop!', 'arrête-toi !']]\n"
     ]
    }
   ],
   "source": [
    "with open(args.source_data_path, encoding=\"utf-8\") as fp:\n",
    "    lines=fp.readlines()\n",
    "lines=[line.replace(\"\\n\",\"\").lower().split(\"\\t\") for line in lines]\n",
    "print(lines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7cb59c-6c01-493f-85e3-881810fdcbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_phrases = (\n",
    "    (\"i\", \"am\"), (\"i\", \"'m\"), \n",
    "    (\"he\", \"is\"), (\"he\", \"'s\"),\n",
    "    (\"she\", \"is\"), (\"she\", \"'s\"),\n",
    "    (\"you\", \"are\"), (\"you\", \"'re\"),\n",
    "    (\"we\", \"are\"), (\"we\", \"'re\"),\n",
    "    (\"they\", \"are\"), (\"they\", \"'re\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad24851b-9c6c-449f-970b-bf160dc874c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for english,french in lines:\n",
    "    data.append({\"english_tokens\":word_tokenize(english,language=\"english\"),\"french_tokens\":word_tokenize(french,language=\"french\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fda02cd8-a211-42af-8570-99377e918b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdataset={phrase: [] for phrase in filter_phrases}\n",
    "for datum in data:\n",
    "    key=tuple(datum[\"english_tokens\"][:2])\n",
    "    if key in subdataset:\n",
    "        subdataset[key].append(datum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "092c4504-4b05-4875-b516-d9a18739a965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({('i', 'am'): 805,\n",
       "  ('i', \"'m\"): 4760,\n",
       "  ('he', 'is'): 1069,\n",
       "  ('he', \"'s\"): 787,\n",
       "  ('she', 'is'): 504,\n",
       "  ('she', \"'s\"): 316,\n",
       "  ('you', 'are'): 449,\n",
       "  ('you', \"'re\"): 2474,\n",
       "  ('we', 'are'): 181,\n",
       "  ('we', \"'re\"): 1053,\n",
       "  ('they', 'are'): 194,\n",
       "  ('they', \"'re\"): 470},\n",
       " 13062)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts ={k:len(v) for k,v in subdataset.items()}\n",
    "counts,sum(counts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176d158-f651-47b2-ab24-de4c63c3b19b",
   "metadata": {},
   "source": [
    "# Sección de división del dataset a entrenamiento, validación y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbda8fde-72a4-48f5-99dd-b837c7f6ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "dataset_stage3=[]\n",
    "for phrase,datum_list in sorted(subdataset.items()):\n",
    "    np.random.shuffle(datum_list)\n",
    "    n_train=int(len(datum_list)*args.perc_train)\n",
    "    n_val=int(len(datum_list)*args.perc_val)\n",
    "    for datum in datum_list[:n_train]:\n",
    "        datum[\"split\"]=\"train\"\n",
    "    for datum in datum_list[n_train:n_train+n_val]:\n",
    "        datum[\"split\"]=\"val\"\n",
    "    for datum in datum_list[n_train+n_val:]:\n",
    "        datum[\"split\"]=\"test\"\n",
    "    dataset_stage3.extend(datum_list)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a6f8e9a-4f4b-4829-8096-83c7f622ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for datum in dataset_stage3:\n",
    "    datum[\"source_language\"]=\" \".join(datum.pop(\"english_tokens\"))\n",
    "    datum[\"target_language\"]=\" \".join(datum.pop(\"french_tokens\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbbc332-68e4-47f2-ae26-0a98e4d6a035",
   "metadata": {},
   "source": [
    "## Creación del datafrma \n",
    "> Previamente se ghizo entoncesw  el etiquetadto de que datos estyán en entrenamiento y que datos están en valdiación a su vez de test, además se agregan las keys de lenguaje guenten a  lenguaje de objetivo\n",
    "> Con esto podemos crearm el dataframe patra ya utilziarlo ewn un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4854065-21df-4240-91a6-cfdaea79079d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>source_language</th>\n",
       "      <th>target_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's the cutest boy in town .</td>\n",
       "      <td>c'est le garçon le plus mignon en ville .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's a nonsmoker .</td>\n",
       "      <td>il est non-fumeur .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's smarter than me .</td>\n",
       "      <td>il est plus intelligent que moi .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's a lovely young man .</td>\n",
       "      <td>c'est un adorable jeune homme .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's three years older than me .</td>\n",
       "      <td>il a trois ans de plus que moi .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split                    source_language  \\\n",
       "0  train     he 's the cutest boy in town .   \n",
       "1  train                he 's a nonsmoker .   \n",
       "2  train            he 's smarter than me .   \n",
       "3  train         he 's a lovely young man .   \n",
       "4  train  he 's three years older than me .   \n",
       "\n",
       "                             target_language  \n",
       "0  c'est le garçon le plus mignon en ville .  \n",
       "1                        il est non-fumeur .  \n",
       "2          il est plus intelligent que moi .  \n",
       "3            c'est un adorable jeune homme .  \n",
       "4           il a trois ans de plus que moi .  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_df=pd.DataFrame(dataset_stage3)\n",
    "nmt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81df979b-7ca2-40e9-92a3-6dbd5a86c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_df.to_csv(args.output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebf3158-00d7-4505-98a0-0d369dcd84a9",
   "metadata": {},
   "source": [
    "# La etapa del modelo\n",
    "> Antes de crear el modelo, necesiatmos generar estructuras de datos que ermitan a pytorhc conocer nuestro modelo, por lo tanto necesiamtos un vectorizador, sin embarog, también necesitamos primero el vocabulario y el vocabualry seuqence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08515015-7851-40ce-86f1-c61021d29da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2f35dae-c0c1-44fb-882b-4adf150f755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\" Clase a procesar texto y extrar el vocabulario para mappear\"\"\"\n",
    "    def __init__(self,token_to_idx=None,add_unk=True,unk_token=\"<UNK>\"):\n",
    "        \"\"\"Args:\n",
    "            token_to_idx (dict): un mapa pre existen de toknes a indices\n",
    "            add_unk(bool): un bandera que indica si se añade el token UNK de desconocido\n",
    "            unk_token(str): el token UNK se añade a el vocabulario\n",
    "            \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx={}\n",
    "        self._token_to_idx=token_to_idx\n",
    "        self._idx_to_token={idx: token\n",
    "                           for token,idx in self._token_to_idx.items()}\n",
    "        self._add_unk=add_unk\n",
    "        self._unk_token=unk_token\n",
    "        self.unk_index=-1\n",
    "        if add_unk:\n",
    "            self.unk_index=self.add_token(unk_token)\n",
    "    def to_serializable(self):\n",
    "        \"\"\" Retorna un diccionario que puede ser serializado \"\"\"\n",
    "        return {\"token_to_idx\":self._token_to_idx,\n",
    "               \"add_unk\":self._add_unk,\n",
    "               \"unk_token\":self._unk_token}\n",
    "    @classmethod\n",
    "    def from_serializable(cls,contents):\n",
    "        \"\"\"instancia el bocabulario desde un diccionario serializado\"\"\"\n",
    "        return cls(**contents)\n",
    "    def add_token(self,token):\n",
    "        \"\"\"\n",
    "        Actualiza los mapeos de diccionarrios basados en los tokens\n",
    "        Args:\n",
    "            token (str): el item a añadir en el vocabulario\n",
    "        Returns:\n",
    "            index(int) : el entero correspondiente al token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index=self._token_to_idx[token]\n",
    "        else:\n",
    "            index=len(self._token_to_idx)\n",
    "            self._token_to_idx[token]=index\n",
    "            self._idx_to_token[index]=token\n",
    "        return index\n",
    "    def lookup_token(self,token):\n",
    "        \"\"\" Obtiene el indice asociado con el tokenn de UNK token si el token no está presente,\n",
    "        Args:\n",
    "            token (str): el token a observar\n",
    "        returns :\n",
    "            index (int) el indices correspondiente al token\n",
    "        Notes:\n",
    "               `unk_index` necesita ser entero positivo (habiendo sido añadido al vocabulario) para la funcionalidad desconocidad\n",
    "               \"\"\"\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token,self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    def lookup_index(self,index):\n",
    "        \"\"\" Obtiene el token asociado al indice\n",
    "        Args:\n",
    "            index (int): el indice a observar\n",
    "        returns :\n",
    "            token (str) el token correspondiendte al indices\n",
    "        Raises:\n",
    "            KeyError: si el indice no está en el vocabulario\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"El indice (%d)no está en el vocabulario\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    def __len__(self):#Refleja la longitudad del vocabulario\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca38c6-2b66-480f-9ec4-73863fa7be90",
   "metadata": {},
   "source": [
    "## secuncia vocabualri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2ec8abf-869d-445b-aad5-497372f1b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1d861-96b5-402a-aeb7-d684b2be100d",
   "metadata": {},
   "source": [
    "# Vectorizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a63648f-f625-470c-9f40-53ca409a4191",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NMTVectorizer(object):\n",
    "    \"\"\"\n",
    "    El vecto4rizador que cordinate el vocabulario y lo coloca para usarlo\n",
    "    \"\"\"\n",
    "    def __init__(self,source_vocab,target_vocab, max_source_length, max_target_length):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        source_vocab(SequenceVocabulary): mapea la fuente de las palabras a enteros\n",
    "        target_vocab(SequenceVocabulary): mapea las palabras obejtivo a enteros\n",
    "        max_source_length(int): la secuencia mas larga en el conjutnod e datos fuente\n",
    "        max_target_length(int): la secuencia mas larga en el conjutnot de datos objetivo\n",
    "        \"\"\"\n",
    "        self.source_vocab=source_vocab\n",
    "        self.target_vocab=target_vocab\n",
    "        self.max_source_length=max_source_length\n",
    "        self.max_target_length=max_target_length\n",
    "    @classmethod\n",
    "    def from_dataframe(cls,bittext_df):\n",
    "        \"\"\"Instancia al vectorizador desde el dataframe de conjunto de datos\n",
    "        args:\n",
    "        bittext_df: (pandas.DataFrame): el texto paralelo al conjunto de datos\n",
    "        Retrun:\n",
    "        una instancia de NMTVectorizer\n",
    "        \"\"\"\n",
    "        source_vocab= SequenceVocabulary()\n",
    "        target_vocab= SequenceVocabulary()\n",
    "        max_source_length,max_target_length=0,0\n",
    "        for _, row in bittext_df.iterrows():\n",
    "            source_tokens=row[\"source_language\"].split(\" \")\n",
    "            if len(source_tokens)>max_source_length:\n",
    "                max_source_length=len(source_tokens)\n",
    "            for token in source_tokens:\n",
    "                source_vocab.add_token(token)\n",
    "            target_tokens=row[\"target_language\"].split(\" \")\n",
    "            if len(target_tokens)>max_target_length:\n",
    "                max_target_length=len(target_tokens)\n",
    "            for token in target_tokens:\n",
    "                target_vocab.add_token(token)\n",
    "        return cls(source_vocab,target_vocab,max_source_length,max_target_length)\n",
    "    def _vectorize(self,indices, vector_length=-1, mask_index=0):\n",
    "        \"\"\"vectoriza los indices dados\n",
    "        args:\n",
    "            indices(list): una lista de enteros que representa una secuencia\n",
    "            vector_length (int): forza la longitrud del vector indice\n",
    "            mask_index (int): el mask_index a usar; casis iemrpe es 0\n",
    "        \"\"\"\n",
    "        if vector_length<0:\n",
    "            vector_length=len(indices)\n",
    "        vector =np.zeros(vector_length,dtype=np.int64)\n",
    "        vector[:len(indices)] =indices\n",
    "        vector[len(indices):]=mask_index\n",
    "        return vector\n",
    "    def _get_source_indices(self,text):\n",
    "        \"\"\"Retorna la fuente de texto vecotrizada\n",
    "        args:\n",
    "            text(str): la fuente de texto, tokens que deberian ser separados por espacios\n",
    "        Returns:\n",
    "            indices(list): lista de neteros que representan el texto\n",
    "            \"\"\"\n",
    "        indices=[self.source_vocab.begin_seq_index]\n",
    "        indices.extend(self.source_vocab.lookup_token(token) for token in text.split(\" \"))\n",
    "        indices.append(self.source_vocab.end_seq_index)\n",
    "        return indices\n",
    "    def _get_target_indices(self,text):\n",
    "        \"\"\"Retorna la fuente del vector\n",
    "\n",
    "        args:\n",
    "            text(str): la fuente de texto; tokjens que deberian ser separados por espacios\n",
    "        returns:\n",
    "            una tupla :(x_indices, y_indices)\n",
    "                x_indices (list): lista de enteros; observaciones en un decoder objetvio\n",
    "                y_indices (lista): una lsit ade enteros, predicciones en un objetivo de decodficiador\n",
    "        \"\"\"\n",
    "        indices=[self.target_vocab.lookup_token(token) for token in text.split(\" \")]\n",
    "        x_indices =[self.target_vocab.begin_seq_index]+indices\n",
    "        y_indices= indices+[self.target_vocab.end_seq_index]\n",
    "        return x_indices,y_indices\n",
    "    def vectorize(self,source_text,target_text,use_dataset_max_lengths=True):\n",
    "        \"\"\"\n",
    "        Retorna la fuente vectorizada y el texto objetivo\n",
    "\n",
    "        args:\n",
    "            source_text (str): texto desde el lneguaje original\n",
    "            target_text (str): texto desde el lenguaje objetiuvo\n",
    "            use_dataset_max_lengths (bool): si el uso del vector maximo de lñongitudes\n",
    "            Retoran \n",
    "            los datos vectorizados como un diccionario con llaves\n",
    "        \"\"\"\n",
    "        source_vector_length=-1\n",
    "        target_vector_length=-1\n",
    "        if use_dataset_max_lengths:\n",
    "            source_vector_length=self.max_source_length+2\n",
    "            target_vector_length=self.max_target_length+1\n",
    "        source_indices=self._get_source_indices(source_text)\n",
    "        source_vector=self._vectorize(source_indices,\n",
    "                                      vector_length=source_vector_length,\n",
    "                                      mask_index=self.source_vocab.mask_index)\n",
    "        target_x_indices, target_y_indices =self._get_target_indices(target_text)\n",
    "        target_x_vector = self._vectorize(target_x_indices,\n",
    "                                          vector_length=target_vector_length,\n",
    "                                          mask_index=self.target_vocab.mask_index)\n",
    "        target_y_vector =self._vectorize(target_y_indices,vector_length=target_vector_length,\n",
    "                                         mask_index=self.target_vocab.mask_index)\n",
    "        return {\n",
    "            \"source_vector\": source_vector,\n",
    "            \"target_x_vector\": target_x_vector,\n",
    "            \"target_y_vector\":target_y_vector,\n",
    "            \"source_length\":len(source_indices)\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab1fd34-6635-4aec-a871-a6103d50b0a7",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a25eecc-75f9-42e1-83c8-6dbebfbc9b45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    # Se hereda la clase Dataset\n",
    "    def __init__(self,text_df,vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_df(pandas.DataFrame): El conjunto de datos\n",
    "            vectorizer(ReviewVectorizer): vectorizer instacinado desde el conjunto de datos\n",
    "        \"\"\"\n",
    "        self.text_df = text_df\n",
    "        print(len(self.text_df))\n",
    "\n",
    "        #self._max_seq_length = max(map(len, self.surname_df.surname)) + 2\n",
    "        self._vectorizer = vectorizer\n",
    "        self.train_df= self.text_df[self.text_df.split==\"train\"]\n",
    "        self.train_size=len(self.train_df)\n",
    "\n",
    "        self.val_df= self.text_df[self.text_df.split==\"val\"]\n",
    "        self.val_size=len(self.val_df)\n",
    "\n",
    "        self.test_df= self.text_df[self.text_df.split==\"test\"]\n",
    "        self.test_size=len(self.test_df)\n",
    "\n",
    "        self._lookup_dic={'train':(self.train_df,self.train_size),\n",
    "                         'val':(self.val_df,self.val_size),\n",
    "                         'test':(self.test_df,self.test_size),\n",
    "                         }\n",
    "        self.set_split(\"train\")\n",
    "        print(\"Total:\", len(self.text_df))\n",
    "        print(\"Train:\", len(self.train_df))\n",
    "        print(\"Val:\", len(self.val_df))\n",
    "        print(\"Test:\", len(self.test_df))\n",
    "        print(self.text_df.split.value_counts())\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls,dataset_csv):\n",
    "        \"\"\"\n",
    "        carga el dataset y hacer un nuevo vectorizador desde cero\n",
    "        args:\n",
    "                review_cvs(str): ubicacion del dataset\n",
    "        Returns:\n",
    "            una instancia de ReviewDataset\n",
    "        \"\"\"\n",
    "        text_df= pd.read_csv(dataset_csv)\n",
    "        train_subset_df = text_df[text_df.split=='train']\n",
    "        return cls(text_df,NMTVectorizer.from_dataframe(train_subset_df))\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"retorna el vectorizador\"\"\"\n",
    "        return self._vectorizer\n",
    "    def set_split(self,split=\"train\"):\n",
    "        \"\"\"\n",
    "        Selecciona la division en el conjunto de datos usando una columna en el dataframe\n",
    "        args:\n",
    "        split(str):  uno de \"train\",\"val\",\"test\"\n",
    "        \"\"\"\n",
    "        self._target_split=split\n",
    "        self._target_df,self._target_size=self._lookup_dic[split]\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"\n",
    "        El punto primario de entrada como metodo para PyTorch en lso conjuntos de datos\n",
    "        Args:\n",
    "        index(int): Es el inice del dato\n",
    "        Returns:\n",
    "            Diccionario de las caracterizitcads de los puntos de datos y labels\n",
    "        \"\"\"\n",
    "        row=self._target_df.iloc[index]\n",
    "        vector_dict = self._vectorizer.vectorize(row.source_language, row.target_language)\n",
    "        return {\"x_source\": vector_dict[\"source_vector\"], \n",
    "                \"x_target\": vector_dict[\"target_x_vector\"],\n",
    "                \"y_target\": vector_dict[\"target_y_vector\"], \n",
    "                \"x_source_length\": vector_dict[\"source_length\"]}\n",
    "    def get_num_batches(self,batch_size):\n",
    "        \"\"\"\n",
    "        Dado un tamaño de batch , retorna el numero de batches del conjunto de datos\n",
    "        Args:\n",
    "            batch_size(int)\n",
    "        Returns:\n",
    "            Numero de batches en el conjuntos\n",
    "        \"\"\"\n",
    "        return len(self)//batch_size\n",
    "def generate_nmt_batches(dataset,batch_size,shuffle=True,drop_last=True,device=\"cpu\"):\n",
    "    \"\"\"Una funciona generador que wrapea el dataloader de pytorch para NMT version\"\"\"\n",
    "    dataloader=DataLoader(dataset=dataset,batch_size=batch_size,drop_last=drop_last,shuffle=shuffle)\n",
    "    for data_dict in dataloader:\n",
    "        lengths=data_dict[\"x_source_length\"].numpy()\n",
    "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
    "        out_data_dict={}\n",
    "        for name,tensor in data_dict.items():\n",
    "            out_data_dict[name]= data_dict[name][sorted_length_indices].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef651ae-5d9a-46c8-bae5-6cfcaff4e138",
   "metadata": {},
   "source": [
    "# Modelo Encoding-Decoding para NMT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3302aec3-3c76-40b7-ba81-2aa5739cb4b1",
   "metadata": {},
   "source": [
    "> El codificador embebe las palabras del idioma de origne y extrae caracteristicas con un bi-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21670da7-face-4eca-895d-05d6e34faa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTEncoder(nn.Module):\n",
    "    def __init__(self,num_embedding,embedding_size,rnn_hidden_size):\n",
    "        \"\"\"\n",
    "        args:\n",
    "        num_embeddings(int): tamaño del vocabulario de orgien\n",
    "        embedding_size (int): tamaño de los vectores de embeddings\n",
    "        rnn_hiddne_size (int): tamaño de los vectores de estados ocultos de los RNN\n",
    "        \"\"\"\n",
    "        super(NMTEncoder,self).__init__()\n",
    "        self.source_embedding = nn.Embedding(num_embedding,embedding_size,padding_idx=0)\n",
    "        self.birnn=nn.GRU(embedding_size,rnn_hidden_size,bidirectional=True,batch_first=True)\n",
    "    def forward(self,x_source,x_lengths):\n",
    "        \"\"\"El paso hacia adelante del modelo\n",
    "        args:\n",
    "            x_source(torch.Tensor): la entrada de datos\n",
    "        x_source.shape(batch,seq_size)\n",
    "        x_lengths (torch.Tensor): vector d elongitudes para cada item en el abtch\n",
    "        Returns:\n",
    "        una tupla : x_unpacked (torch.Tesnor),x:_birnn_h (torch.Tensor)\n",
    "        \"\"\"\n",
    "        x_embedded=self.source_embedding(x_source)\n",
    "        x_lengths=x_lengths.detach().cpu().numpy()\n",
    "        x_packed =pack_padded_sequence(x_embedded,x_lengths,batch_first=True)\n",
    "        x_birnn_out ,x_birnn_h =self.birnn(x_packed)\n",
    "        x_birnn_h=x_birnn_h.permute(1,0,2)\n",
    "        x_birnn_h=x_birnn_h.contiguous().view(x_birnn_h.size(0),-1)\n",
    "        x_unpacked, _ = pad_packed_sequence(x_birnn_out,batch_first=True)\n",
    "        return x_unpacked,x_birnn_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf7efd-7e35-4a27-8a80-cbc59ed6f2c7",
   "metadata": {},
   "source": [
    "### Sistema de atencion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5326998-4089-462a-8ed2-f5a8c77b7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose_attention(encoder_state_vectors,query_vector):\n",
    "    \"\"\"encoider_state_vectors: 3dim tsnfro de bigru en le encoder\n",
    "    query_evctor: un estado oculto del decoder de GRU\n",
    "    \"\"\"\n",
    "    batch_size,num_vectors,vector_size =encoder_state_vectors.size()\n",
    "    vector_scores= torch.sum(encoder_state_vectors*query_vector.view(batch_size,1,vector_size),dim=2)\n",
    "    vector_probabilities =F.softmax(vector_scores,dim=1)\n",
    "    weighted_vectors =encoder_state_vectors*vector_probabilities.view(batch_size,num_vectors,1)\n",
    "    context_vectors=torch.sum(weighted_vectors,dim=1)\n",
    "    return context_vectors,vector_probabilities,vector_scores\n",
    "def terse_attention(encoder_state_vectors,query_vector):\n",
    "    \"\"\"\n",
    "    encoder_state_vectors: 3dim tensor from bi-GRU in encoder\n",
    "    query_vector: hidden state\n",
    "    \"\"\"\n",
    "    vector_scores =torch.matmul(encoder_state_vectors,query_vector.unsqueeze(dim=2)).squeeze()\n",
    "    vector_probabilites=F.softmax(vector_scores,dim=-1)\n",
    "    context_vectors = torch.matmul(encoder_state_vectors.transpose(-2,-1),\n",
    "                                   vector_probabilities.unsqueeze(dim=2)).squeeze()\n",
    "    return context_vectors,vector_probabilites\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f38fa-e472-43ba-b796-e282fab272d1",
   "metadata": {},
   "source": [
    "## Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b6524f5-e1ae-4b29-9855-e6bcbcc56772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDecoder(nn.Module):\n",
    "    def __init__(self, num_embedding, embedding_size, rnn_hidden_size, bos_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_embeddings (int):Numero de embeddings que tambien es el numero de palabras unicas de la fuente objetiuvo\n",
    "            embedding_size (int): tamaño del vector de embedding\n",
    "            rnn_hidden_size (int): tamaño del estado oculto de la RNN\n",
    "            bos_index(int): begin-of-sequence indices\n",
    "        \"\"\"\n",
    "        super(NMTDecoder, self).__init__()\n",
    "        self._rnn_hidden_size = rnn_hidden_size\n",
    "        self.target_embedding = nn.Embedding(num_embeddings=num_embedding, \n",
    "                                             embedding_dim=embedding_size, \n",
    "                                             padding_idx=0)\n",
    "        self.gru_cell = nn.GRUCell(embedding_size + rnn_hidden_size, \n",
    "                                   rnn_hidden_size)\n",
    "        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_embedding)\n",
    "        self.bos_index = bos_index\n",
    "        self._sampling_temperature = 3\n",
    "    def _init_indices(self, batch_size):\n",
    "        \"\"\" return the BEGIN-OF-SEQUENCE index vector \"\"\"\n",
    "        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index\n",
    "    \n",
    "    def _init_context_vectors(self, batch_size):\n",
    "        \"\"\" return a zeros vector for initializing the context \"\"\"\n",
    "        return torch.zeros(batch_size, self._rnn_hidden_size)\n",
    "            \n",
    "    def forward(self, encoder_state, initial_hidden_state, target_sequence, sample_probability=0.0):\n",
    "        \"\"\"El paso hacia adelante del modelo\n",
    "        \n",
    "        Args:\n",
    "            encoder_state (torch.Tensor): la salida del NMTEncoder\n",
    "            initial_hidden_state (torch.Tensor): El ultimo estado oculto del  NMTEncoder\n",
    "            target_sequence (torch.Tensor): el tensor de datos del texto objetivo\n",
    "            sample_probability (float): el esquema dek muestreo de parametros\n",
    "                de porbabilidades de usar las prediccioones del modelo en cada decoder paso\n",
    "        Returns:\n",
    "            output_vectors (torch.Tensor): vectores de predicción para cada paso en la salida\n",
    "        \"\"\"\n",
    "        if target_sequence is None:\n",
    "            sample_probability = 1.0\n",
    "        else:\n",
    "            # We are making an assumption there: The batch is on first\n",
    "            # The input is (Batch, Seq)\n",
    "            # We want to iterate over sequence so we permute it to (S, B)\n",
    "            target_sequence = target_sequence.permute(1, 0)\n",
    "            output_sequence_size = target_sequence.size(0)\n",
    "        \n",
    "        # use the provided encoder hidden state as the initial hidden state\n",
    "        h_t = self.hidden_map(initial_hidden_state)\n",
    "        \n",
    "        batch_size = encoder_state.size(0)\n",
    "        # initialize context vectors to zeros\n",
    "        context_vectors = self._init_context_vectors(batch_size)\n",
    "        # initialize first y_t word as BOS\n",
    "        y_t_index = self._init_indices(batch_size)\n",
    "        \n",
    "        h_t = h_t.to(encoder_state.device)\n",
    "        y_t_index = y_t_index.to(encoder_state.device)\n",
    "        context_vectors = context_vectors.to(encoder_state.device)\n",
    "\n",
    "        output_vectors = []\n",
    "        self._cached_p_attn = []\n",
    "        self._cached_ht = []\n",
    "        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\n",
    "        \n",
    "        for i in range(output_sequence_size):\n",
    "            # Schedule sampling is whe\n",
    "            use_sample = np.random.random() < sample_probability\n",
    "            if not use_sample:\n",
    "                y_t_index = target_sequence[i]\n",
    "                \n",
    "            # Step 1: Embed word and concat with previous context\n",
    "            y_input_vector = self.target_embedding(y_t_index)\n",
    "            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)\n",
    "            \n",
    "            # Step 2: Make a GRU step, getting a new hidden vector\n",
    "            h_t = self.gru_cell(rnn_input, h_t)\n",
    "            self._cached_ht.append(h_t.cpu().detach().numpy())\n",
    "            \n",
    "            # Step 3: Use the current hidden to attend to the encoder state\n",
    "            context_vectors, p_attn, _ = verbose_attention(encoder_state_vectors=encoder_state, \n",
    "                                                           query_vector=h_t)\n",
    "            \n",
    "            # auxillary: cache the attention probabilities for visualization\n",
    "            self._cached_p_attn.append(p_attn.cpu().detach().numpy())\n",
    "            \n",
    "            # Step 4: Use the current hidden and context vectors to make a prediction to the next word\n",
    "            prediction_vector = torch.cat((context_vectors, h_t), dim=1)\n",
    "            score_for_y_t_index = self.classifier(F.dropout(prediction_vector, 0.3))\n",
    "            \n",
    "            if use_sample:\n",
    "                p_y_t_index = F.softmax(score_for_y_t_index * self._sampling_temperature, dim=1)\n",
    "                # _, y_t_index = torch.max(p_y_t_index, 1)\n",
    "                y_t_index = torch.multinomial(p_y_t_index, 1).squeeze()\n",
    "            \n",
    "            # auxillary: collect the prediction scores\n",
    "            output_vectors.append(score_for_y_t_index)\n",
    "            \n",
    "        output_vectors = torch.stack(output_vectors).permute(1, 0, 2)\n",
    "        \n",
    "        return output_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3556e2d-7ba8-4648-8f99-4c4778badad7",
   "metadata": {},
   "source": [
    "> El modelo NMTmodel coordina y encapsula la parte encoder y decoder en un solo forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9fa6c5b3-dc43-49ba-9b9b-68e7dc151c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTModel(nn.Module):\n",
    "    def __init__(self,source_vocab_size, source_embedding_size, \n",
    "                 target_vocab_size,target_embedding_size, encoding_size, target_bos_index):\n",
    "        \"\"\"args\n",
    "\n",
    "        source_vocab_size (int): numero de palabras unicas en el idioma fuente\n",
    "        source_embedding_size (int): tamaño de los vectores de embeddings de la fuente\n",
    "        target_vocab_size (int) : numero de palabras unicas en el idioma objetivo\n",
    "        target_embedding_size (int): tamaño de los vecotres de embeddings objetivo\n",
    "        encodign_size(int): tamaño del encoder RNN\n",
    "        target_bos_index (int): indicespara BEGIN-OF-SEQUENCE token\n",
    "        \"\"\"\n",
    "        super(NMTModel,self).__init__()\n",
    "        self.encoder= NMTEncoder(num_embedding=source_vocab_size,\n",
    "                                 embedding_size=source_embedding_size,\n",
    "                                 rnn_hidden_size=encoding_size)\n",
    "        decoding_size= encoding_size*2\n",
    "        self.decoder=NMTDecoder(num_embedding=target_vocab_size,\n",
    "                                 embedding_size=target_embedding_size,\n",
    "                                 rnn_hidden_size=decoding_size,\n",
    "                                   bos_index=target_bos_index)\n",
    "    def forward(self,x_source, x_source_lengths, target_sequence,sample_probability=0.0):\n",
    "        \"\"\" el paso hacioa adelante del modelo\n",
    "        args:\n",
    "        x_source (torch.Tensor): la fuente de datos de origen del tensor\n",
    "            x_source.shape sera de (batch,vectorizer.max_sequence_lenghts)\n",
    "        x_source_lengths (torch.Tensor): la longitu de las secuencias en x_source\n",
    "        target_sequence(torch.Tensor): el objetivo del tensor de datos de texto\n",
    "    Returns:\n",
    "        decoded_states (torch.Tensor): las predicciones de los vecotres en el paso de salida\n",
    "        \"\"\"\n",
    "        encoder_state, final_hidden_states=self.encoder(x_source,x_source_lengths)\n",
    "        decoded_states= self.decoder(encoder_state=encoder_state, initial_hidden_state=final_hidden_states,\n",
    "                                    target_sequence=target_sequence,\n",
    "                                    sample_probability=sample_probability)\n",
    "        return decoded_states\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c82e2e4-245e-4b92-b3f7-b2d18ab0cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "    \n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "         \n",
    "        # If loss worsened\n",
    "        if loss_t >= loss_tm1:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def normalize_sizes(y_pred, y_true):\n",
    "    \"\"\"Normalize tensor sizes\n",
    "    \n",
    "    Args:\n",
    "        y_pred (torch.Tensor): the output of the model\n",
    "            If a 3-dimensional tensor, reshapes to a matrix\n",
    "        y_true (torch.Tensor): the target predictions\n",
    "            If a matrix, reshapes to be a vector\n",
    "    \"\"\"\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6668ed3b-7015-4893-b026-391ed764788a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel\\vectorizer.json\n",
      "\tmodel\\model.pth\n",
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "args = Namespace(dataset_csv=\"data/nmt/simplest_eng_fra.csv\",\n",
    "                 vectorizer_file=\"vectorizer.json\",\n",
    "                 model_state_file=\"model.pth\",\n",
    "                 save_dir=\"model\",\n",
    "                 reload_from_files=False,\n",
    "                 expand_filepaths_to_save_dir=True,\n",
    "                 cuda=True,\n",
    "                 seed=1337,\n",
    "                 learning_rate=5e-4,\n",
    "                 batch_size=32,\n",
    "                 num_epochs=100,\n",
    "                 early_stopping_criteria=5,              \n",
    "                 source_embedding_size=24, \n",
    "                 target_embedding_size=24,\n",
    "                 encoding_size=32,\n",
    "                 catch_keyboard_interrupt=True)\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e5605b6-79dd-4219-8b07-1d12505411bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13062\n",
      "Total: 13062\n",
      "Train: 9138\n",
      "Val: 1955\n",
      "Test: 1969\n",
      "split\n",
      "train    9138\n",
      "test     1969\n",
      "val      1955\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "    # training from a checkpoint\n",
    "    dataset = NMTDataset.load_dataset_and_load_vectorizer(args.dataset_csv,\n",
    "                                                          args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = NMTDataset.load_dataset_and_make_vectorizer(args.dataset_csv)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58547ea9-1dc4-4a6c-967e-8b240e01f818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model\n"
     ]
    }
   ],
   "source": [
    "model = NMTModel(source_vocab_size=len(vectorizer.source_vocab), \n",
    "                 source_embedding_size=args.source_embedding_size, \n",
    "                 target_vocab_size=len(vectorizer.target_vocab),\n",
    "                 target_embedding_size=args.target_embedding_size, \n",
    "                 encoding_size=args.encoding_size,\n",
    "                 target_bos_index=vectorizer.target_vocab.begin_seq_index)\n",
    "\n",
    "if args.reload_from_files and os.path.exists(args.model_state_file):\n",
    "    model.load_state_dict(torch.load(args.model_state_file))\n",
    "    print(\"Reloaded model\")\n",
    "else:\n",
    "    print(\"New model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4f49aac-a3b6-4895-82a8-cf69bb0cb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(args.device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "mask_index = vectorizer.target_vocab.mask_index\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "dataset.set_split('train')\n",
    "\n",
    "dataset.set_split('val')\n",
    "\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        sample_probability = (20 + epoch_index) / args.num_epochs\n",
    "        \n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_nmt_batches(dataset, \n",
    "                                               batch_size=args.batch_size, \n",
    "                                               device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------    \n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = model(batch_dict['x_source'], \n",
    "                           batch_dict['x_source_length'], \n",
    "                           batch_dict['x_target'],\n",
    "                           sample_probability=sample_probability)\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "\n",
    "            # -----------------------------------------\n",
    "            # compute the running loss and running accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            \n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_nmt_batches(dataset, \n",
    "                                               batch_size=args.batch_size, \n",
    "                                               device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = model(batch_dict['x_source'], \n",
    "                           batch_dict['x_source_length'], \n",
    "                           batch_dict['x_target'],\n",
    "                           sample_probability=sample_probability)\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "            # compute the running loss and accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            \n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=model, \n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "        \n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "86385fc1-1070-49a8-9688-b9669741ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "chencherry = bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1ea3b9b6-ed7f-4260-b303-606194ce0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_from_indices(indices, vocab, strict=True, return_string=True):\n",
    "    ignore_indices = set([vocab.mask_index, vocab.begin_seq_index, vocab.end_seq_index])\n",
    "    out = []\n",
    "    for index in indices:\n",
    "        if index == vocab.begin_seq_index and strict:\n",
    "            continue\n",
    "        elif index == vocab.end_seq_index and strict:\n",
    "            break\n",
    "        else:\n",
    "            out.append(vocab.lookup_index(index))\n",
    "    if return_string:\n",
    "        return \" \".join(out)\n",
    "    else:\n",
    "        return out\n",
    "    \n",
    "class NMTSampler:\n",
    "    def __init__(self, vectorizer, model):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.model = model\n",
    "    \n",
    "    def apply_to_batch(self, batch_dict):\n",
    "        self._last_batch = batch_dict\n",
    "        y_pred = self.model(x_source=batch_dict['x_source'], \n",
    "                            x_source_lengths=batch_dict['x_source_length'], \n",
    "                            target_sequence=batch_dict['x_target'])\n",
    "        self._last_batch['y_pred'] = y_pred\n",
    "        \n",
    "        attention_batched = np.stack(self.model.decoder._cached_p_attn).transpose(1, 0, 2)\n",
    "        self._last_batch['attention'] = attention_batched\n",
    "        \n",
    "    def _get_source_sentence(self, index, return_string=True):\n",
    "        indices = self._last_batch['x_source'][index].cpu().detach().numpy()\n",
    "        vocab = self.vectorizer.source_vocab\n",
    "        return sentence_from_indices(indices, vocab, return_string=return_string)\n",
    "\n",
    "    def _get_reference_sentence(self, index, return_string=True):\n",
    "        indices = self._last_batch['y_target'][index].cpu().detach().numpy()\n",
    "        vocab = self.vectorizer.target_vocab\n",
    "        return sentence_from_indices(indices, vocab, return_string=return_string)\n",
    "    \n",
    "    def _get_sampled_sentence(self, index, return_string=True):\n",
    "        _, all_indices = torch.max(self._last_batch['y_pred'], dim=2)\n",
    "        sentence_indices = all_indices[index].cpu().detach().numpy()\n",
    "        vocab = self.vectorizer.target_vocab\n",
    "        return sentence_from_indices(sentence_indices, vocab, return_string=return_string)\n",
    "\n",
    "    def get_ith_item(self, index, return_string=True):\n",
    "        output = {\"source\": self._get_source_sentence(index, return_string=return_string), \n",
    "                  \"reference\": self._get_reference_sentence(index, return_string=return_string), \n",
    "                  \"sampled\": self._get_sampled_sentence(index, return_string=return_string),\n",
    "                  \"attention\": self._last_batch['attention'][index]}\n",
    "        \n",
    "        reference = output['reference']\n",
    "        hypothesis = output['sampled']\n",
    "        \n",
    "        if not return_string:\n",
    "            reference = \" \".join(reference)\n",
    "            hypothesis = \" \".join(hypothesis)\n",
    "        \n",
    "        output['bleu-4'] = bleu_score.sentence_bleu(references=[reference],\n",
    "                                                    hypothesis=hypothesis,\n",
    "                                                    smoothing_function=chencherry.method1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e11469c6-c98e-4818-ad5f-d1e2ee376c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval().to(args.device)\n",
    "\n",
    "sampler = NMTSampler(vectorizer, model)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_nmt_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       device=args.device)\n",
    "\n",
    "test_results = []\n",
    "for batch_dict in batch_generator:\n",
    "    sampler.apply_to_batch(batch_dict)\n",
    "    for i in range(args.batch_size):\n",
    "        test_results.append(sampler.get_ith_item(i, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b06902d1-3072-43b8-9f6f-9d801c52842d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.4092395009232586), np.float64(0.41345446951562914))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([r['bleu-4'] for r in test_results], bins=100);\n",
    "np.mean([r['bleu-4'] for r in test_results]), np.median([r['bleu-4'] for r in test_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7574a0eb-95b6-46be-8418-8cbac4c4bf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': \"i 'm sorry that you 've been badly injured .\",\n",
       " 'truth': 'je suis désolé que vous ayez été gravement blessé .',\n",
       " 'sampled': 'je suis désolé de vous avoir avoir été ça .'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_source_sentence(vectorizer, batch_dict, index):\n",
    "    indices = batch_dict['x_source'][index].cpu().data.numpy()\n",
    "    vocab = vectorizer.source_vocab\n",
    "    return sentence_from_indices(indices, vocab)\n",
    "\n",
    "def get_true_sentence(vectorizer, batch_dict, index):\n",
    "    return sentence_from_indices(batch_dict['y_target'].cpu().data.numpy()[index], vectorizer.target_vocab)\n",
    "    \n",
    "def get_sampled_sentence(vectorizer, batch_dict, index):\n",
    "    y_pred = model(x_source=batch_dict['x_source'], \n",
    "                   x_source_lengths=batch_dict['x_source_length'], \n",
    "                   target_sequence=batch_dict['x_target'], \n",
    "                   sample_probability=1.0)\n",
    "    return sentence_from_indices(torch.max(y_pred, dim=2)[1].cpu().data.numpy()[index], vectorizer.target_vocab)\n",
    "\n",
    "def get_all_sentences(vectorizer, batch_dict, index):\n",
    "    return {\"source\": get_source_sentence(vectorizer, batch_dict, index), \n",
    "            \"truth\": get_true_sentence(vectorizer, batch_dict, index), \n",
    "            \"sampled\": get_sampled_sentence(vectorizer, batch_dict, index)}\n",
    "    \n",
    "def sentence_from_indices(indices, vocab, strict=True):\n",
    "    ignore_indices = set([vocab.mask_index, vocab.begin_seq_index, vocab.end_seq_index])\n",
    "    out = []\n",
    "    for index in indices:\n",
    "        if index == vocab.begin_seq_index and strict:\n",
    "            continue\n",
    "        elif index == vocab.end_seq_index and strict:\n",
    "            return \" \".join(out)\n",
    "        else:\n",
    "            out.append(vocab.lookup_index(index))\n",
    "    return \" \".join(out)\n",
    "\n",
    "results = get_all_sentences(vectorizer, batch_dict, 1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87eac03-b8f8-44dc-b39c-9592aba538bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
