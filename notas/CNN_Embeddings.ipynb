{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89cb4ce-ae4f-4c36-93f2-29de575ac86c",
   "metadata": {},
   "source": [
    "## Transfer learning on CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f3bb1f2-451a-4cb4-8a9d-a0b2650dddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np\n",
    "class NewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, news_df, vectorizer):\n",
    "        self.news_df = news_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, news_df.title)) + 2\n",
    "\n",
    "        self.train_df = self.news_df[self.news_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.news_df[self.news_df.split == 'val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.news_df[self.news_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            'train': (self.train_df, self.train_size),\n",
    "            'val': (self.val_df, self.validation_size),\n",
    "            'test': (self.test_df, self.test_size)\n",
    "        }\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        class_counts = news_df.category.value_counts().to_dict()\n",
    "\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.category_vocab.lookup_token(item[0])\n",
    "\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, news_csv):\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        train_names_df = news_df[news_df.split == \"train\"]\n",
    "        return cls(news_df, NewsVectorizer.from_dataframe(train_names_df))\n",
    "\n",
    "         \n",
    "    @classmethod   \n",
    "    def load_dataset_and_make_vectorizer(cls,news_csv):\n",
    "        \"\"\" Load dataset and make a new vectorizer from scratch\n",
    "        Args:\n",
    "            news_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of NewsDataset\n",
    "        \"\"\"\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        train_names_df = news_df[news_df.split==\"train\"]\n",
    "        return cls(news_df, NewsVectorizer.from_dataframe(train_names_df))\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"The primary entry point method for PyTorch datasets\n",
    "        Args:\n",
    "            index (int): the index tp the data point\n",
    "        Returns:\n",
    "            a dict holding the data point's features (x_data) and label (y_target)\n",
    "            \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        title_vector= self._vectorizer.vectorize(row.title, self._max_seq_length)\n",
    "        category_index = self._vectorizer.category_vocab.lookup_token(row.category)\n",
    "        return {\"x_data\": title_vector,\n",
    "                \"y_target\": category_index\n",
    "                    }\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9485f1cf-a5d7-4c00-a38a-394a79cd24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" Clase a procesar texto y extrar el vocabulario para mappear\"\"\"\n",
    "    def __init__(self,token_to_idx=None,mask_token=\"<MASK>\",add_unk=True,unk_token=\"<UNK>\"):\n",
    "        \"\"\"Args:\n",
    "            token_to_idx (dict): un mapa pre existen de toknes a indices\n",
    "            add_unk(bool): un bandera que indica si se añade el token UNK de desconocido\n",
    "            unk_token(str): el token UNK se añade a el vocabulario\n",
    "            \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx={}\n",
    "        self._token_to_idx=token_to_idx\n",
    "        self._idx_to_token={idx: token\n",
    "                           for token,idx in self._token_to_idx.items()}\n",
    "        self._add_unk=add_unk\n",
    "        self._unk_token=unk_token\n",
    "        self._mask_token=mask_token\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index=-1\n",
    "        if add_unk:\n",
    "            self.unk_index=self.add_token(unk_token)\n",
    "    def to_serializable(self):\n",
    "        \"\"\" Retorna un diccionario que puede ser serializado \"\"\"\n",
    "        return {\"token_to_idx\":self._token_to_idx,\n",
    "               \"add_unk\":self._add_unk,\n",
    "               \"unk_token\":self._unk_token,\n",
    "               \"mask_token\":self._mask_token}\n",
    "    @classmethod\n",
    "    def from_serializable(cls,contents):\n",
    "        \"\"\"instancia el bocabulario desde un diccionario serializado\"\"\"\n",
    "        return cls(**contents)\n",
    "    def add_token(self,token):\n",
    "        \"\"\"\n",
    "        Actualiza los mapeos de diccionarrios basados en los tokens\n",
    "        Args:\n",
    "            token (str): el item a añadir en el vocabulario\n",
    "        Returns:\n",
    "            index(int) : el entero correspondiente al token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index=self._token_to_idx[token]\n",
    "        else:\n",
    "            index=len(self._token_to_idx)\n",
    "            self._token_to_idx[token]=index\n",
    "            self._idx_to_token[index]=token\n",
    "        return index\n",
    "    def lookup_token(self,token):\n",
    "        \"\"\" Obtiene el indice asociado con el tokenn de UNK token si el token no está presente,\n",
    "        Args:\n",
    "            token (str): el token a observar\n",
    "        returns :\n",
    "            index (int) el indices correspondiente al token\n",
    "        Notes:\n",
    "               `unk_index` necesita ser entero positivo (habiendo sido añadido al vocabulario) para la funcionalidad desconocidad\n",
    "               \"\"\"\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token,self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    def lookup_index(self,index):\n",
    "        \"\"\" Obtiene el token asociado al indice\n",
    "        Args:\n",
    "            index (int): el indice a observar\n",
    "        returns :\n",
    "            token (str) el token correspondiendte al indices\n",
    "        Raises:\n",
    "            KeyError: si el indice no está en el vocabulario\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"El indice (%d)no está en el vocabulario\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    def __len__(self):#Refleja la longitudad del vocabulario\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "814469cd-84cf-4168-9505-32091a22fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75d83be8-db08-4202-8113-2ead745b65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
    "    def __init__(self, title_vocab, category_vocab):\n",
    "        self.title_vocab = title_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "\n",
    "    def vectorize(self, title, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            title (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            the vetorized title (numpy.array)\n",
    "        \"\"\"\n",
    "        indices = [self.title_vocab.begin_seq_index]\n",
    "        indices.extend(self.title_vocab.lookup_token(token) \n",
    "                       for token in title.split(\" \"))\n",
    "        indices.append(self.title_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.title_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, news_df, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            news_df (pandas.DataFrame): the target dataset\n",
    "            cutoff (int): frequency threshold for including in Vocabulary \n",
    "        Returns:\n",
    "            an instance of the NewsVectorizer\n",
    "        \"\"\"\n",
    "        category_vocab = Vocabulary()        \n",
    "        for category in sorted(set(news_df.category)):\n",
    "            category_vocab.add_token(category)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for title in news_df.title:\n",
    "            for token in title.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "        \n",
    "        title_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                title_vocab.add_token(word)\n",
    "        \n",
    "        return cls(title_vocab, category_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        title_vocab = \\\n",
    "            SequenceVocabulary.from_serializable(contents['title_vocab'])\n",
    "        category_vocab =  \\\n",
    "            Vocabulary.from_serializable(contents['category_vocab'])\n",
    "\n",
    "        return cls(title_vocab=title_vocab, category_vocab=category_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'title_vocab': self.title_vocab.to_serializable(),\n",
    "                'category_vocab': self.category_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66b37e2e-a0d3-4611-96f6-f4baf90527c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_from_file(glove_filepath):\n",
    "    \"\"\"Load the Glove embeddings\n",
    "\n",
    "    Args:\n",
    "    glove_filepath (str): áth to the glove embedding fike\n",
    "    Returns:\n",
    "        word_to_index (dict): embeddings \n",
    "    \"\"\"\n",
    "    word_to_index={}\n",
    "    embeddings=[]\n",
    "    with open(glove_filepath,\"r\",encoding=\"utf-8\") as fp:\n",
    "        for index,line in enumerate(fp):\n",
    "            line=line.split(\" \")\n",
    "            word_to_index[line[0]] = index\n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "def make_embedding_matrix(glove_filepath,words):\n",
    "    \"\"\" Create embedding matrix for a specigic set of words.\n",
    "    Args:\n",
    "        glove_filepath (str): file path to the glove embeddings\n",
    "        words( list) : list of words in the dataset\n",
    "    Returns:\n",
    "        final_embeddings (numpy.ndarray): embedding matrix\n",
    "    \"\"\"\n",
    "    word_to_idx ,glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size= glove_embeddings.shape[1]\n",
    "    final_embeddings= np.zeros((len(words),embedding_size))\n",
    "    for i,word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i,:] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1,embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i,:]=embedding_i\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2517e592-5e68-446c-9150-e26f8067d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self,embedding_size,num_embeddings,num_channels,hidden_dim, num_classes,dropout_p,\n",
    "                pretrained_embeddings=None, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size (int): size of the embedding vectors\n",
    "            num_embeddings (int): number of embedding vectors\n",
    "            filter_width (int): width of the convolutional kernels\n",
    "            num_channels (int): number of convolutional kernels per layer\n",
    "            hidden_dim (int): the size of the hidden dimension\n",
    "            num_classes (int): the number of classes in classification\n",
    "            dropout_p (float): a dropout parameter \n",
    "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
    "                default is None. If provided, \n",
    "            padding_idx (int): an index representing a null position\n",
    "        \"\"\"\n",
    "        super(NewsClassifier,self).__init__()\n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb=nn.Embedding(embedding_dim=embedding_size,num_embeddings=num_embeddings,padding_idx=padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings=torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb=nn.Embedding(embedding_dim=embedding_size,num_embeddings=num_embeddings,padding_idx=padding_idx,\n",
    "                                  _weight=pretrained_embeddings)\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embedding_size,\n",
    "                      out_channels=num_channels,\n",
    "                      kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels,kernel_size=3,stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels,out_channels=num_channels, kernel_size=3,stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels,out_channels=num_channels,kernel_size=3),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.training=True\n",
    "        self._dropout_p =dropout_p\n",
    "        self.fc1 = nn.Linear(num_channels,hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim,num_classes)\n",
    "    def forward(self,x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, dataset._max_seq_length)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
    "        \"\"\"\n",
    "        #Embed and permute so features are channels\n",
    "        x_embedded =self.emb(x_in).permute(0,2,1)\n",
    "        features= self.convnet(x_embedded)\n",
    "        remaining_size=features.size(dim=2)\n",
    "        features= F.avg_pool1d(features,remaining_size).squeeze(dim=2)\n",
    "        features= F.dropout(features,p=self._dropout_p,training=self.training)\n",
    "        ##Final linear layer to produc classification outputs\n",
    "\n",
    "        intermediate_vector=F.relu(F.dropout(self.fc1(features),\n",
    "                                             p=self._dropout_p,training=self.training))\n",
    "        prediction_vector= self.fc2(intermediate_vector)\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector,dim=1)\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e2f7699-0f69-4e1e-bf18-cc433ea9fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    news_csv=\"data/ag_news/news_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/ch5/document_classification\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='data/wiki_giga_2024_300_MFT20_vectors_seed_2024_alpha_0.75_eta_0.05_combined.txt', \n",
    "    use_glove=True,\n",
    "    embedding_size=300, \n",
    "    hidden_dim=100, \n",
    "    num_channels=100, \n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.1, \n",
    "    batch_size=128, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff556cb6-691f-41b4-8d0e-05d83d77aa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "#El conjunto de datos instanciado\n",
    "import torch.optim as optim\n",
    "dataset=NewsDataset.load_dataset_and_make_vectorizer(args.news_csv)\n",
    "vectorizer= dataset.get_vectorizer()\n",
    "if args.use_glove:\n",
    "    words = vectorizer.title_vocab._token_to_idx.keys()\n",
    "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath, \n",
    "                                       words=words)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "classifier = NewsClassifier(embedding_size=args.embedding_size, \n",
    "                            num_embeddings=embeddings.shape[0],\n",
    "                            num_channels=args.num_channels,\n",
    "                            hidden_dim=args.hidden_dim, \n",
    "                            num_classes=len(vectorizer.category_vocab), \n",
    "                            dropout_p=args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda=False\n",
    "args.device= torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c354513c-df14-4ed1-81b5-7e5079419af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(classifier.parameters(),lr=args.learning_rate)\n",
    "def make_train_state(args):\n",
    "    return { \"epoch_index\":0,\n",
    "             \"train_loss\": [],\n",
    "             \"train_acc\":[],\n",
    "             \"val_loss\":[],\n",
    "             \"val_acc\":[],\n",
    "             \"test_loss\":-1,\n",
    "             \"test_acc\":-1}\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "def generate_batches(dataset,batch_size,shuffle=True,\n",
    "                     drop_last=True,device=\"cpu\"):\n",
    "    \"\"\"Una funcion generation la que wrapea ek dataloader de pytorxh. SE asegurarar de que cada tesnor este en un unico dispositivo\n",
    "    \"\"\"\n",
    "    dataloader=DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9bccf08-55af-4900-8a2f-4cd27b012a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_state=make_train_state(args)\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index']=epoch_index\n",
    "    # Itera sobre nuestro dataset\n",
    "    #Configura: el generador de batch, la perdidad, el accuracy a 0 y el conjunto de entrenamiento a activo\n",
    "    dataset.set_split(\"train\")\n",
    "    batch_generator=generate_batches(dataset,batch_size=args.batch_size,device=args.device)\n",
    "    running_loss=0.0\n",
    "    running_acc=0.0\n",
    "    classifier.train()\n",
    "    for batch_index,batch_dict in enumerate(batch_generator):\n",
    "        \n",
    "        # La rutina de entrenamiento se conforma de 5 pasos\n",
    "\n",
    "        #paso 1, los gradientes en cero\n",
    "        optimizer.zero_grad()\n",
    "        #Paso . calcula la salida\n",
    "        y_pred=classifier(batch_dict[\"x_data\"])\n",
    "        #paso 3, calcula la perdidad\n",
    "        loss=loss_func(y_pred,batch_dict[\"y_target\"])\n",
    "        loss_batch=loss.to(\"cpu\").item()\n",
    "        running_loss+=(loss_batch-running_loss)/(batch_index+1)\n",
    "        #paso 4, usa la perdidad para producir el gradiente\n",
    "        loss.backward()\n",
    "        #Paso 5, usa el otpimizar para que tomo el paso gradiente\n",
    "        optimizer.step()\n",
    "\n",
    "        ##Calcula el accuracy\n",
    "        acc_batch=compute_accuracy(y_pred,batch_dict[\"y_target\"])\n",
    "        running_acc+=(acc_batch-running_acc)/(batch_index+1)\n",
    "    train_state[\"train_loss\"].append(running_loss)\n",
    "    train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "    #Iterar sobre el dataset de validacion\n",
    "    #Configura, el generator de batch, la perdiddad, el accuracy a 0 y modo eval\n",
    "    dataset.set_split(\"val\")\n",
    "    batch_generator=generate_batches(dataset,batch_size=args.batch_size,device=args.device)\n",
    "    running_loss=0.0\n",
    "    running_acc=0.0\n",
    "    classifier.eval()\n",
    "    #Empieza la evaluacion\n",
    "    for batch_index,batch_dict in enumerate(batch_generator):\n",
    "        # Paso 1: calcula la salida\n",
    "        y_pred=classifier(batch_dict[\"x_data\"])\n",
    "        # paso 2, calcula la salida\n",
    "        loss = loss_func(y_pred,batch_dict['y_target'])\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch-running_loss)/(batch_index+1)\n",
    "        #paso 3, calcula el accuracy\n",
    "        acc_batch= compute_accuracy(y_pred,batch_dict['y_target'])\n",
    "        running_acc+=(acc_batch-running_acc)/(batch_index+1)\n",
    "    train_state[\"val_loss\"].append(running_loss)\n",
    "    train_state[\"val_acc\"].append(running_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "759a6763-7365-43c3-b0a1-4f2c1f3b5a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comienza la evaluacion\n",
      "batch:  0\n",
      "batch:  1\n",
      "batch:  2\n",
      "batch:  3\n",
      "batch:  4\n",
      "batch:  5\n",
      "batch:  6\n",
      "batch:  7\n",
      "batch:  8\n",
      "batch:  9\n",
      "batch:  10\n",
      "batch:  11\n",
      "batch:  12\n",
      "batch:  13\n",
      "batch:  14\n",
      "batch:  15\n",
      "batch:  16\n",
      "batch:  17\n",
      "batch:  18\n",
      "batch:  19\n",
      "batch:  20\n",
      "batch:  21\n",
      "batch:  22\n",
      "batch:  23\n",
      "batch:  24\n",
      "batch:  25\n",
      "batch:  26\n",
      "batch:  27\n",
      "batch:  28\n",
      "batch:  29\n",
      "batch:  30\n",
      "batch:  31\n",
      "batch:  32\n",
      "batch:  33\n",
      "batch:  34\n",
      "batch:  35\n",
      "batch:  36\n",
      "batch:  37\n",
      "batch:  38\n",
      "batch:  39\n",
      "batch:  40\n",
      "batch:  41\n",
      "batch:  42\n",
      "batch:  43\n",
      "batch:  44\n",
      "batch:  45\n",
      "batch:  46\n",
      "batch:  47\n",
      "batch:  48\n",
      "batch:  49\n",
      "batch:  50\n",
      "batch:  51\n",
      "batch:  52\n",
      "batch:  53\n",
      "batch:  54\n",
      "batch:  55\n",
      "batch:  56\n",
      "batch:  57\n",
      "batch:  58\n",
      "batch:  59\n",
      "batch:  60\n",
      "batch:  61\n",
      "batch:  62\n",
      "batch:  63\n",
      "batch:  64\n",
      "batch:  65\n",
      "batch:  66\n",
      "batch:  67\n",
      "batch:  68\n",
      "batch:  69\n",
      "batch:  70\n",
      "batch:  71\n",
      "batch:  72\n",
      "batch:  73\n",
      "batch:  74\n",
      "batch:  75\n",
      "batch:  76\n",
      "batch:  77\n",
      "batch:  78\n",
      "batch:  79\n",
      "batch:  80\n",
      "batch:  81\n",
      "batch:  82\n",
      "batch:  83\n",
      "batch:  84\n",
      "batch:  85\n",
      "batch:  86\n",
      "batch:  87\n",
      "batch:  88\n",
      "batch:  89\n",
      "batch:  90\n",
      "batch:  91\n",
      "batch:  92\n",
      "batch:  93\n",
      "batch:  94\n",
      "batch:  95\n",
      "batch:  96\n",
      "batch:  97\n",
      "batch:  98\n",
      "batch:  99\n",
      "batch:  100\n",
      "batch:  101\n",
      "batch:  102\n",
      "batch:  103\n",
      "batch:  104\n",
      "batch:  105\n",
      "batch:  106\n",
      "batch:  107\n",
      "batch:  108\n",
      "batch:  109\n",
      "batch:  110\n",
      "batch:  111\n",
      "batch:  112\n",
      "batch:  113\n",
      "batch:  114\n",
      "batch:  115\n",
      "batch:  116\n",
      "batch:  117\n",
      "batch:  118\n",
      "batch:  119\n",
      "batch:  120\n",
      "batch:  121\n",
      "batch:  122\n",
      "batch:  123\n",
      "batch:  124\n",
      "batch:  125\n",
      "batch:  126\n",
      "batch:  127\n",
      "batch:  128\n",
      "batch:  129\n",
      "batch:  130\n",
      "batch:  131\n",
      "batch:  132\n",
      "batch:  133\n",
      "batch:  134\n",
      "batch:  135\n",
      "batch:  136\n",
      "batch:  137\n",
      "batch:  138\n",
      "batch:  139\n"
     ]
    }
   ],
   "source": [
    "dataset.set_split(\"test\")\n",
    "batch_generator=generate_batches(dataset,batch_size=args.batch_size,device=args.device)\n",
    "running_loss=0.\n",
    "running_acc=0.\n",
    "classifier.eval()\n",
    "print(\"comienza la evaluacion\")\n",
    "for batch_index,batch_dict in enumerate(batch_generator):\n",
    "    print(\"batch: \",batch_index)\n",
    "    #Compute the output\n",
    "    y_pred=classifier(batch_dict[\"x_data\"])\n",
    "    # calcula la perdidad\n",
    "    loss = loss_func(y_pred,batch_dict[\"y_target\"])\n",
    "    loss_batch= loss.item()\n",
    "    running_loss += (loss_batch-running_loss)/(batch_index+1)\n",
    "    #Calcula el accuracy \n",
    "    acc_batch= compute_accuracy(y_pred,batch_dict[\"y_target\"])\n",
    "    running_acc+=(acc_batch-running_acc)/(batch_index+1)\n",
    "train_state[\"test_loss\"]= running_loss\n",
    "train_state[\"test_acc\"] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7fd3e55-30f3-40af-a393-3ccd6bcd22d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69.99213986280489, 80.69979039634144, 81.80259146341469, 82.51119474085367, 83.16025152439023, 84.0867949695122, 84.83112614329264, 85.86961699695121, 86.68302210365864, 87.42854420731705, 88.32412347560985, 89.0625, 89.87828696646349, 90.45588795731709, 91.07636242378057, 91.65158155487798, 92.15891768292676, 92.49118711890245, 92.97946836890245, 93.38200266768287, 93.69402629573175, 93.99771341463409, 94.2168445121951, 94.62771532012196, 94.63962461890249, 94.93973894817081, 95.15053353658534, 95.37204649390245, 95.3815739329269, 95.49947599085361, 95.78291730182931, 95.80673589939025, 95.96155678353662, 96.07945884146338, 96.25095274390249, 96.27119855182934, 96.39029153963413, 96.42363757621932, 96.43197408536575, 96.66420541158546, 96.70350609756099, 96.71303353658539, 96.7737709603659, 96.87857278963415, 96.8654725609756, 96.99171112804882, 96.98694740853648, 97.06078506097569, 97.00600228658527, 97.11318597560971, 97.15129573170735, 97.15605945121942, 97.25967035060985, 97.22513338414632, 97.1405773628049, 97.27038871951211, 97.39543635670735, 97.35613567073166, 97.32874428353648, 97.46689214939016, 97.42282774390247, 97.41568216463415, 97.50262004573172, 97.3561356707317, 97.48356516768291, 97.48118330792684, 97.5383479420731, 97.62290396341463, 97.56216653963415, 97.56931211890236, 97.62409489329261, 97.5895579268292, 97.50976562500001, 97.63481326219517, 97.52762957317074, 97.5597846798781, 97.6538681402439, 97.7157964939025, 97.6669683689026, 97.6348132621951, 97.65267721036584, 97.59789443597572, 97.6324314024391, 97.72056021341471, 97.67292301829256, 97.82417111280485, 97.66696836890243, 97.71460556402432, 97.81821646341456, 97.71936928353661, 97.69555068597562, 97.70269626524382, 97.752715320122, 97.73008765243898, 97.7812976371951, 97.78367949695125, 97.7908250762194, 97.80035251524403, 97.79320693597556, 97.77296112804868]\n",
      "[79.15736607142854, 79.63727678571428, 79.86049107142857, 79.72656250000001, 79.03459821428567, 78.96205357142857, 79.29687500000003, 79.13504464285717, 78.86718750000001, 77.8794642857143, 78.28125000000001, 77.74553571428572, 77.80133928571429, 77.37723214285712, 78.10267857142856, 77.18191964285715, 77.20424107142858, 77.10379464285711, 76.9363839285714, 76.46763392857136, 76.71316964285718, 76.81361607142857, 76.52343750000001, 76.86383928571432, 76.74665178571432, 76.68526785714286, 76.75781250000003, 76.4174107142857, 76.25000000000001, 76.37276785714282, 76.1886160714286, 75.87611607142861, 76.03794642857143, 76.12723214285708, 76.08816964285715, 75.89843750000003, 76.34486607142854, 75.84263392857137, 76.03236607142856, 76.2834821428571, 75.90959821428567, 76.71875000000003, 76.1439732142857, 75.95424107142861, 76.38950892857143, 76.06026785714289, 75.89843750000001, 76.07700892857137, 76.17745535714286, 76.19977678571426, 75.91517857142854, 76.17187500000003, 76.46205357142857, 75.78125000000001, 76.33928571428571, 76.57366071428572, 75.81473214285711, 76.64062500000001, 76.03794642857147, 76.4564732142858, 75.90959821428574, 75.77008928571428, 76.02120535714286, 76.25000000000001, 76.34486607142857, 76.02120535714282, 76.26674107142858, 75.703125, 76.13839285714285, 76.0379464285714, 76.0770089285714, 75.9095982142857, 76.12723214285714, 76.04352678571426, 76.04910714285711, 75.94308035714286, 76.02120535714285, 75.93191964285714, 76.16629464285715, 76.25558035714285, 76.07700892857142, 76.01004464285714, 76.0658482142857, 76.37276785714283, 76.13281249999999, 76.11607142857144, 75.86495535714285, 76.11049107142856, 76.11607142857143, 76.36718749999999, 75.91517857142857, 75.93191964285711, 75.68638392857144, 75.7924107142857, 76.09375000000001, 76.06026785714285, 76.05468750000004, 75.73660714285714, 76.1439732142857, 76.09375000000004]\n",
      "76.57366071428575\n"
     ]
    }
   ],
   "source": [
    "print(train_state[\"train_acc\"])\n",
    "print(train_state[\"val_acc\"])\n",
    "print(train_state[\"test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1118fe95-7c78-488d-a68f-29ef642223f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "def predict_category(title,classifier,vectorizer,max_length):\n",
    "    \"\"\"Predits a news category for a new title\n",
    "    Args:\n",
    "        title(str): a raw title string\n",
    "        classifier (NewsClassifier): an instane of the trained classifier\n",
    "        vectorizer(NewsVectorizer): the corresponding vectorizer\n",
    "        max_length(int): the max sequence length\n",
    "        \"\"\"\n",
    "    title = preprocess_text(title)\n",
    "    vectorized_title = \\\n",
    "        torch.tensor(vectorizer.vectorize(title, vector_length=max_length))\n",
    "    result = classifier(vectorized_title.unsqueeze(0), apply_softmax=True)\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    predicted_category = vectorizer.category_vocab.lookup_index(indices.item())\n",
    "\n",
    "    return {'category': predicted_category, \n",
    "            'probability': probability_values.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2fc75e4f-562d-4d7e-b080-057d3012a3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Category: Business\n",
      "==============================\n",
      "Prediction: World (p=0.86)\n",
      "\t + Sample: AZ suspends marketing of cancer drug\n",
      "Prediction: Sports (p=0.79)\n",
      "\t + Sample: Business world has mixed reaction to Perez move\n",
      "Prediction: Sports (p=1.00)\n",
      "\t + Sample: Betting Against Bombay\n",
      "Prediction: Sci/Tech (p=1.00)\n",
      "\t + Sample: Malpractice Insurers Face a Tough Market\n",
      "Prediction: Sports (p=0.69)\n",
      "\t + Sample: NVIDIA Is Vindicated\n",
      "------------------------------\n",
      "\n",
      "True Category: Sci/Tech\n",
      "==============================\n",
      "Prediction: Sci/Tech (p=1.00)\n",
      "\t + Sample: Spies prize webcam #39;s eyes\n",
      "Prediction: Sci/Tech (p=1.00)\n",
      "\t + Sample: Sober worm causes headaches\n",
      "Prediction: Business (p=1.00)\n",
      "\t + Sample: Local Search: Missing Pieces Falling into Place\n",
      "Prediction: Sci/Tech (p=1.00)\n",
      "\t + Sample: Hackers baiting Internet users with Beckham pix\n",
      "Prediction: Sports (p=1.00)\n",
      "\t + Sample: Nokia adds BlackBerry support to Series 80 handsets\n",
      "------------------------------\n",
      "\n",
      "True Category: Sports\n",
      "==============================\n",
      "Prediction: Sports (p=0.79)\n",
      "\t + Sample: Is Meyer the man to get Irish up?\n",
      "Prediction: World (p=0.99)\n",
      "\t + Sample: Who? Who? And Clemens\n",
      "Prediction: Sports (p=1.00)\n",
      "\t + Sample: Baseball Today (AP)\n",
      "Prediction: Sci/Tech (p=1.00)\n",
      "\t + Sample: Mark Kreidler: Yao Ming epitomizes the Chinese athlete who is &lt;b&gt;...&lt;/b&gt;\n",
      "Prediction: Sports (p=1.00)\n",
      "\t + Sample: No. 5 Miami Rebounds to Beat FSU in Overtime\n",
      "------------------------------\n",
      "\n",
      "True Category: World\n",
      "==============================\n",
      "Prediction: Business (p=1.00)\n",
      "\t + Sample: Arafat in pain but expected to recover-Shaath\n",
      "Prediction: World (p=1.00)\n",
      "\t + Sample: Maoist rebels bomb Kathmandu building, no injuries (Reuters)\n",
      "Prediction: World (p=1.00)\n",
      "\t + Sample: Son Running for Ill. Rep.'s House Seat (AP)\n",
      "Prediction: Sports (p=0.82)\n",
      "\t + Sample: Strong Quake Hits in Japan\n",
      "Prediction: World (p=1.00)\n",
      "\t + Sample: Israel assassinates Hamas militant in Damascus\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_samples():\n",
    "    samples = {}\n",
    "    for cat in dataset.val_df.category.unique():\n",
    "        samples[cat] = dataset.val_df.title[dataset.val_df.category==cat].tolist()[:5]\n",
    "    return samples\n",
    "\n",
    "val_samples = get_samples()\n",
    "#title = input(\"Enter a news title to classify: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "for truth, sample_group in val_samples.items():\n",
    "    print(f\"True Category: {truth}\")\n",
    "    print(\"=\"*30)\n",
    "    for sample in sample_group:\n",
    "        prediction = predict_category(sample, classifier, \n",
    "                                      vectorizer, dataset._max_seq_length + 1)\n",
    "        print(\"Prediction: {} (p={:0.2f})\".format(prediction['category'],\n",
    "                                                  prediction['probability']))\n",
    "        print(\"\\t + Sample: {}\".format(sample))\n",
    "    print(\"-\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bcc247-336a-4595-acbc-c0e0e29831c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
